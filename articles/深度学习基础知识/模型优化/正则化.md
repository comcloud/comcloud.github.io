>  正则化（`Regularization`）是一种在机器学习中用来防止过拟合的技术。过拟合是指模型在训练数据上表现很好，但在新数据上的泛化能力较差。正则化通过**在损失函数中增加额外的惩罚项**，限制模型的复杂度，从而提升模型的泛化能力。

1. **L1 正则化**：也称为 Lasso 正则化，它通过在模型的损失函数中增加权重的 L1 范数（权重向量的绝对值之和）来实现正则化。[L1 正则化](https://so.csdn.net/so/search?q=L1 正则化&spm=1001.2101.3001.7020)倾向于产生稀疏权重矩阵，即将一些权重推向零，从而实现特征选择的效果。
2. **L2 正则化**：也称为 Ridge 正则化，它通过在模型的损失函数中增加权重的 L2 范数（权重向量的平方和）来实现正则化。L2 正则化会使权重值变得较小，但不会直接导致权重稀疏，因此不具有特征选择的作用，但可以有效地控制模型的复杂度。
3. **Elastic Net 正则化**：Elastic Net 是 L1 和 L2 正则化的组合，它在损失函数中同时使用 L1 和 L2 范数，可以综合两者的优点。
4. **Dropout**：Dropout 是一种特殊的正则化技术，通过在训练过程中随机地丢弃（将其权重置为零）网络中的部分神经元，以及它们的连接，来减少神经网络的复杂度。这样可以防止神经元之间的共适应性，从而减少过拟合。
5. **早停（Early Stopping）**：早停是一种简单而有效的正则化方法，它在训练过程中监视模型在验证集上的性能，一旦验证集上的性能开始下降，就停止训练。这样可以避免模型在训练集上过拟合。
6. **数据增强（Data Augmentation）**：数据增强是通过对训练数据进行变换来增加数据的多样性，从而减少过拟合的风险。例如，在图像分类任务中可以进行随机裁剪、旋转、翻转等操作来增加训练数据的数量和多样性。
8. **权重衰减（Weight Decay）**：权重衰减是一种通过在损失函数中增加权重的平方和或绝对值之和来实现正则化的技术。它等价于对权重参数进行 L2 正则化。



#### 1. **L1 正则化（Lasso 正则化）**
   - **原理**：L1 正则化通过在损失函数中加入参数的**绝对值**之和来限制模型复杂度，形式为：
     $$
     L(\theta) = \text{Loss}(\theta) + \lambda \sum_{i=1}^{n} |\theta_i|
     $$
     
     其中，\( $\theta$ \) 是模型参数，\( $\lambda$ \) 是正则化参数。
   - **作用**：L1 正则化具有**稀疏性**，即它倾向于将一些参数的权重推向零。
   - **应用**：常用于特征选择，尤其是在特征数量非常大的情况下。

>  L1 正则化能够**自动进行特征选择**，去掉不重要的特征。原因在于**绝对值函数的导数在零点处不连续**，这意味着在优化过程中，如果某个参数的绝对值小到一定程度，L1 正则化会强制它变为零，从而**自动丢弃不重要的特征**。
>
> **几何直观：** L1 正则化在参数空间中的惩罚是一个**菱形**（在二维空间下），其顶点位于坐标轴上，这意味着最优解容易位于坐标轴上（即某些参数值为零）。因此，L1 正则化倾向于将部分参数推向零，从而实现特征选择。

<img src="/Users/rayss/Public/work/AI求职/study/深度学习基础知识/模型优化/image/L1.png" alt="image-20250315105347708" style="zoom:17%;" />

~~~python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Lasso

np.random.seed(1)
X = np.random.rand(100, 1) 
y = 3 * X.squeeze() + np.random.normal(0, 0.3, 100)

linear_model = LinearRegression()
linear_model.fit(X, y)

lasso_model = Lasso(alpha=0.2) 
lasso_model.fit(X, y)

plt.figure(figsize=(12, 6))

plt.scatter(X, y, color='black', label='Data')
plt.plot(X, linear_model.predict(X), color='red', linewidth=2, label='Linear Regression')
plt.plot(X, lasso_model.predict(X), color='green', linewidth=2, label='Lasso Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('L1 Regularization')
plt.legend()

plt.savefig('./L1.png', dpi=300)
plt.show()
~~~



#### 2. **L2 正则化（Ridge 正则化）**
   - **原理**：L2 正则化通过在损失函数中加入参数的**平方和**来限制模型复杂度，形式为：
     $$
     L(\theta) = \text{Loss}(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2
     $$
     
   - **作用**：L2 正则化不会将权重完全推向零，而是**缩小权重值**。
   - **应用**：常用于回归问题中，尤其是在模型参数不多的情况下。

> L2正则化有助于**避免模型对某些特征过度依赖**，从而提升模型的稳定性和泛化能力。
>
> **几何直观：** L2 正则化在参数空间中的惩罚是一个**圆形**（在二维空间下）。与 L1 正则化的菱形不同，圆形的最优解通常不会正好位于坐标轴上，这意味着它倾向于将参数值**均匀缩小**，而不是使某些参数为零。

<img src="/Users/rayss/Public/work/AI求职/study/深度学习基础知识/模型优化/image/L2.png" alt="image-20250315105347709" style="zoom: 20%;" />

---

> L1和L2的差异的根本原因在于**绝对值函数**和**平方函数**的数学性质不同。绝对值函数在零点的导数不连续，这会导致某些参数的权重变为零，而平方函数的导数则是连续的，它倾向于均匀地缩小所有参数值，而不是使某些参数完全为零

---



#### 3. **弹性网正则化（Elastic Net）**
   - **原理**：弹性网正则化结合了 L1 和 L2 正则化的特点。它在损失函数中同时加入 L1 和 L2 正则化项，形式为：
     $$
     L(\theta) = \text{Loss}(\theta) + \lambda_1 \sum_{i=1}^{n} |\theta_i| + \lambda_2 \sum_{i=1}^{n} \theta_i^2
     $$
     
   - **作用**：弹性网正则化同时具备 **L1 的稀疏性**和 **L2 的稳定性**，因此它在处理大量特征时具有较好的表现，尤其是在特征间存在相关性的情况下。
   - **应用**：常用于特征选择和高维数据中。

<img src="/Users/rayss/Public/work/AI求职/study/深度学习基础知识/模型优化/image/E.png" alt="image-20250315105347710" style="zoom:17%;" />

~~~python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression,Lasso ,Ridge, ElasticNet

# 生成带有噪声的线性数据集
np.random.seed(0)
X = np.random.rand(100, 1)  # 特征
y = 3 * X.squeeze() + np.random.normal(0, 0.3, 100)  # 标签

# 没有使用 L2 正则化的线性回归模型
linear_model = LinearRegression()
linear_model.fit(X, y)

lasso_model = Lasso()
lasso_model.fit(X, y)

# 使用 L2 正则化的 Ridge 回归模型
ridge_model = Ridge(alpha=1.0)  # 正则化参数 alpha
ridge_model.fit(X, y)

elastic_model = ElasticNet()
elastic_model.fit(X,y)

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, linear_model.predict(X), color='red', linewidth=2, label='Linear Regression (No Regularization)')
# plt.plot(X, lasso_model.predict(X), color='purple', linewidth=2, label='L1 Regression')
# plt.plot(X, ridge_model.predict(X), color='green', linewidth=2, label='Ridge Regression (L2 Regularization)')
plt.plot(X, elastic_model.predict(X), color='black', linewidth=2, label='Elastic Regression (E Regularization)')
plt.xlabel('X')
plt.ylabel('y')
plt.title('E Regularization')
plt.legend()
plt.savefig("./E.png", dpi=300)
plt.show()
~~~



#### 4. **早停（Early Stopping）**
   - **原理**：早停是一种通过**监控训练误差和验证误差**的变化来控制训练过程的方法。如果**在训练过程中，验证误差开始增加，而训练误差继续减小，就会停止训练**，防止过拟合。
   - **作用**：避免模型在训练数据上过度拟合，通过找到合适的训练时机，确保模型在未见过的数据上表现良好。

~~~python
import torch
import torch.optim as optim

class EarlyStopping:
    def __init__(self, patience=5, min=0, verbose=False, path='checkpoint.pt'):
        """
        EarlyStopping 类用于实现早停机制
        - patience: 容忍多少个 epoch 验证集损失没有改进
        - min: 验证集损失的最小变化量（只有超过此值才算有改进）
        - verbose: 是否打印早停信息
        - path: 保存最优模型的路径
        """
        self.patience = patience
        self.min = min
        self.verbose = verbose
        self.path = path
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
        self.best_model = None

    def __call__(self, val_loss, model):
        """
        调用此方法来检查是否应该提前停止训练
        - val_loss: 当前 epoch 验证集上的损失
        - model: 当前训练的模型
        """
        if self.best_loss - val_loss > self.min:
            self.best_loss = val_loss
            self.counter = 0
            # 保存最优模型
            self.best_model = model.state_dict()
        else:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print("Early stopping triggered")
        return self.early_stop

# 训练过程中的早停实现
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):
    # train code... 
        # 检查是否触发早停
        if early_stopping(avg_val_loss, model):
            print("Early stopping...")
            model.load_state_dict(early_stopping.best_model)  # 恢复最好的模型
            break

    return model
~~~



#### 5. **Dropout**
   - **原理**：Dropout 是一种常用的正则化方法，特别是在深度神经网络中。它在训练过程中**随机“丢弃”神经网络中的一些神经元**，即在每次训练时，随机选择一部分神经元的输出设置为零，从而减少神经网络对某些特定神经元的依赖。
   - **作用**：通过随机丢弃神经元，防止神经网络过度依赖某些特征，提高泛化能力。

~~~python
nn.Dropout(0.5)
~~~



#### 6. **数据增强（Data Augmentation）**
   - **原理**：数据增强是一种通过对训练数据进行变换（如旋转、平移、裁剪、缩放等）来生成新的训练样本，从而增加数据的多样性，防止模型对特定训练数据的过拟合。
   - **作用**：通过人为增加训练数据的多样性，使模型更好地泛化到未见过的数据。

#### 7. **权重衰减（Weight Decay）**
   - **原理**：权重衰减是一种通过在损失函数中增加权重的平方和来约束模型的参数，使得权重值变得更小，从而减少过拟合的可能性。
   - **作用**：与 L2 正则化相似，权重衰减有助于防止模型过于复杂，避免过拟合。



>  正则化方法中的超参数 λ 控制正则化项的权重。较大的 λ 会施加更强的正则化，从而使模型更加简化；较小的 λ 则允许模型更复杂。需要通过交叉验证等方法来选择一个合适的 λ 值，以平衡模型的拟合能力和泛化能力。

