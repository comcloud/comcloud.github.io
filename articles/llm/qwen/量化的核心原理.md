---
title: "量化的核心原理"
description: "深入讲解神经网络量化的数学原理、权重量化运算机制"
date: 2024-08-03
tags: ["Qwen", "量化", "数学原理", "深度学习"]
category: "LLM"
layout: markdown
---

### 量化的核心原理：浮点数到整数的映射

量化的基本思想是将神经网络中通常使用的浮点数（如FP32，32位浮点数）表示的权重和激活值，映射到低位宽的整数（如INT8，8位整数）表示。这个映射过程需要定义一个**量化函数**和一个**反量化函数**。

最常见的量化方法是**线性均匀量化 (Linear Uniform Quantization)**，它通过一个**缩放因子 (Scale Factor, \S\)** 和一个**零点 (Zero Point, \Z\)** 来实现浮点数和整数之间的线性映射。

#### 1. 量化参数 (**S** 和 **Z**) 的确定

在进行量化之前，我们需要确定浮点数范围（$F_{min}$,$F_{max}$）和整数范围（$Q_{min}$,$Q_{max}$）。

- **整数范围**通常是固定的。例如，对于 8 位无符号整数` (UINT8)`$Q_{min}=0$,$Q_{max}=255$；对于 8 位有符号整数 `(INT8)`，$Q_{min}=-128$,$Q_{max}=127$。
- **浮点数范围**则需要从模型中获取。对于权重，可以通过其最小值和最大值直接确定。对于激活值，由于它们是动态变化的，通常需要在推理前通过**校准 (Calibration)** 过程，用一小部分代表性数据运行模型，收集激活值的统计信息（如最小值、最大值或均值、方差）来确定。

一旦确定了浮点数的范围 ($F_{min}$,$F_{max}$） 和整数的范围 ($Q_{min}$,$Q_{max}$），就可以计算 S 和 Z：

- 缩放因子 S (Scale Factor):
  $$
  S=(Q_{max}−Q_{min})/ (F_{max}−F_{min})
  $$
  

  这个 S 表示浮点数单位的变化对应整数单位的变化量。

- 零点 Z (Zero Point):

  Z=Qmin−SFmin

  零点 Z 是为了将浮点数 0.0 映射到某个整数值。它是一个整数，通过四舍五入得到。

#### 2. 量化过程 (Quantization)

将一个浮点数 FP 量化为整数 Q 的公式：

Q=round(SFP+Z)

其中 round 表示四舍五入。

#### 3. 反量化过程 (Dequantization)

在某些情况下，为了进行计算或输出，需要将整数 Q 反量化回浮点数 FPreconstructed。

FPreconstructed=(Q−Z)×S

**为什么需要零点 \**Z\**？**

零点的存在是为了确保浮点数 0.0 能够精确地映射到一个整数值（通常是 Qmin 或 Qmax 或中间某个值），这对于激活函数（如 ReLU，其中大量值为 0）和稀疏权重矩阵非常重要，可以避免量化误差累积。

### 权重量化运算如何进行？

模型参数（权重）的量化比激活值相对简单，因为权重是固定的。主要有两种策略：

#### 1. 离线量化 (Offline Quantization) / PTQ (Post-Training Quantization)

这是最常见的权重量化方式，特别是在部署阶段。

1. **加载预训练的 FP32 模型。**

2. 逐层或逐通道地计算每个权重张量的 **Fmin** 和 **Fmax**。

   - 例如，对于一个卷积核 

     W∈RCout×Cin×KH×KW

     ：

     - **逐层量化：** 找到整个 W 的最小值 Wmin 和最大值 Wmax。
     - **逐通道量化：** 对于每个输出通道 Cout，单独计算其对应权重的最小值和最大值，得到 Cout 组 (Sc,Zc)。这种方式精度通常更高。

3. **根据 \**Fmin,Fmax\** 和目标整数范围，计算出对应的 \**S\** 和 \**Z\**。**

4. **使用 \**S\** 和 \**Z\** 将每个浮点权重 \**Wfp\** 量化为整数 \**Wq\**。** Wq=round(SWfp+Z)

5. **存储量化后的模型：** 此时模型的权重都是 INT8 (或 INT4) 整数。

**推理时权重的运算：**

在推理时，量化后的权重 Wq 会直接参与计算。但在实际的硬件或推理引擎中，运算通常仍然是浮点运算（或者更精确地说是模拟浮点运算）。

考虑一个简单的乘法：Y=W×X

在量化后，变成：Yfp≈(Wq−Zw)×Sw×(Xq−Zx)×Sx

这个浮点乘法可以分解为整数运算：

Yfp≈(Wq⋅Xq−Wq⋅Zx−Xq⋅Zw+Zw⋅Zx)⋅Sw⋅Sx

为了避免频繁的反量化和浮点乘法，高性能的推理引擎（如 TensorRT）会进行**算子融合 (Operator Fusion)** 和**整数化计算 (Integer-only Arithmetic)**。它们会把 Sw⋅Sx 作为一个**重标度因子 (rescale factor)** 传递，并在中间结果中累积。最终的输出会在一个更高的整数精度（如 INT32）中进行累加，然后通过最终的缩放和零点转换为下一个 INT8 激活值或最终的 FP32 输出。

**核心思想是：** 尽量在整数域内进行计算，只在必要时（如层与层之间传递激活值或最终输出）进行反量化和重新量化。

#### 2. 在线量化 (Online Quantization) / QAT (Quantization-Aware Training)

QAT 则是在训练过程中引入量化操作的模拟。

1. 在训练图 (Computation Graph) 中插入伪量化 (Fake Quantization) 节点。
   - 这些伪量化节点在正向传播时，会执行 **量化 -> 反量化** 的操作。也就是说，浮点权重先被量化成低精度整数，然后立即反量化回浮点数。
   - 这样，模型在训练时，其浮点权重虽然仍然是 FP32，但它们会“感知”到低精度量化带来的误差，并学习如何去弥补这些误差。
2. **反向传播时，梯度仍然是针对 FP32 权重计算的。** 伪量化操作通常是可导的（或者通过梯度旁路，Straight-Through Estimator, STE）。
3. **训练结束后，直接保存模型中的 FP32 权重。** 这些权重已经适应了量化误差。
4. **在部署时，直接对这些训练好的 FP32 权重进行最终的量化**（类似 PTQ 的步骤 2-4），将它们转换为 INT8。由于权重已经适应了量化，所以精度损失很小。

**总结权重量化运算的原理：**

无论是 PTQ 还是 QAT，核心都是找到一个合适的线性映射关系（由 S 和 Z 定义），将原始浮点数范围映射到目标整数范围。在推理时，通过硬件支持的整数乘加指令，结合预先计算好的 S 和 Z，可以在整数域内高效地完成大部分计算，从而达到加速和减少资源占用的目的。对于大模型来说，由于其庞大的参数量，即使是 8-bit 或 4-bit 量化，也能带来巨大的内存和计算效益。