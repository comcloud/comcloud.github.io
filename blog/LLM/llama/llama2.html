<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Llama 2 全面学习指南</title>
    <style>
        :root {
            --primary: #1877F2; /* Meta蓝色 */
            --secondary: #65676B;
            --accent: #00A6FF;
            --light: #F0F2F5;
            --dark: #1C1E21;
            --success: #42B72A;
            --warning: #FFBA00;
            --danger: #FA3E3E;
            --radius: 12px;
            --shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            --transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', 'PingFang SC', 'Microsoft YaHei', sans-serif;
            line-height: 1.7;
            color: var(--dark);
            background-color: var(--light);
            overflow-x: hidden;
            scroll-behavior: smooth;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 25px;
        }

        /* Header Styles */
        header {
            background: linear-gradient(135deg, var(--primary), #166FE5);
            color: white;
            padding: 100px 0 120px;
            position: relative;
            overflow: hidden;
            text-align: center;
            margin-bottom: 60px;
            clip-path: polygon(0 0, 100% 0, 100% 90%, 0 100%);
        }

        header::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" preserveAspectRatio="none"><path fill="rgba(255,255,255,0.05)" d="M0,0 L100,0 L100,100 L0,100 Z" /></svg>');
            background-size: cover;
            opacity: 0.3;
        }

        h1 {
            font-size: 3.2rem;
            margin-bottom: 20px;
            position: relative;
            animation: fadeInDown 1s ease;
            font-weight: 700;
            text-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .subtitle {
            font-size: 1.3rem;
            opacity: 0.9;
            max-width: 800px;
            margin: 0 auto 30px;
            position: relative;
            animation: fadeIn 1.5s ease;
            font-weight: 300;
        }

        .scroll-down {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            color: white;
            font-size: 1.8rem;
            animation: bounce 2s infinite;
            cursor: pointer;
            z-index: 10;
        }

        /* Navigation */
        .nav-container {
            position: sticky;
            top: 0;
            z-index: 100;
            background-color: rgba(255, 255, 255, 0.98);
            box-shadow: var(--shadow);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            transition: var(--transition);
        }

        .nav-container.scrolled {
            box-shadow: 0 6px 30px rgba(0, 0, 0, 0.1);
        }

        nav {
            display: flex;
            justify-content: center;
            padding: 18px 0;
            max-width: 1200px;
            margin: 0 auto;
        }

        nav a {
            color: var(--dark);
            text-decoration: none;
            margin: 0 18px;
            font-weight: 500;
            padding: 8px 12px;
            border-radius: var(--radius);
            transition: var(--transition);
            position: relative;
            font-size: 1rem;
        }

        nav a:hover {
            color: var(--primary);
            background: rgba(24, 119, 242, 0.1);
        }

        nav a.active {
            color: var(--primary);
        }

        nav a::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            width: 0;
            height: 3px;
            background: var(--primary);
            transition: var(--transition);
            transform: translateX(-50%);
            border-radius: 3px 3px 0 0;
        }

        nav a:hover::after, nav a.active::after {
            width: calc(100% - 24px);
        }

        /* Section Styles */
        section {
            padding: 80px 0;
            position: relative;
        }

        section:nth-child(even) {
            background-color: white;
        }

        section::before {
            content: '';
            position: absolute;
            top: -60px;
            left: 0;
            width: 100%;
            height: 60px;
            background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1200 120" preserveAspectRatio="none"><path fill="%23F0F2F5" d="M0,0V46.29c47.79,22.2,103.59,32.17,158,28,70.36-5.37,136.33-33.31,206.8-37.5C438.64,32.43,512.34,53.67,583,72.05c69.27,18,138.3,24.88,209.4,13.08,36.15-6,69.85-17.84,104.45-29.34C989.49,25,1113-14.29,1200,52.47V0Z" opacity=".25"/><path fill="%23F0F2F5" d="M0,0V15.81C13,36.92,27.64,56.86,47.69,72.05,99.41,111.27,165,111,224.58,91.58c31.15-10.15,60.09-26.07,89.67-39.8,40.92-19,84.73-46,130.83-49.67,36.26-2.85,70.9,9.42,98.6,31.56,31.77,25.39,62.32,62,103.63,73,40.44,10.79,81.35-6.69,119.13-24.28s75.16-39,116.92-43.05c59.73-5.85,113.28,22.88,168.9,38.84,30.2,8.66,59,6.17,87.09-7.5,22.43-10.89,48-26.93,60.65-49.24V0Z" opacity=".5"/><path fill="%23F0F2F5" d="M0,0V5.63C149.93,59,314.09,71.32,475.83,42.57c43-7.64,84.23-20.12,127.61-26.46,59-8.63,112.48,12.24,165.56,35.4C827.93,77.22,886,95.24,951.2,90c86.53-7,172.46-45.71,248.8-84.81V0Z"/></svg>');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            z-index: 1;
        }

        section:nth-child(even)::before {
            background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1200 120" preserveAspectRatio="none"><path fill="%23ffffff" d="M0,0V46.29c47.79,22.2,103.59,32.17,158,28,70.36-5.37,136.33-33.31,206.8-37.5C438.64,32.43,512.34,53.67,583,72.05c69.27,18,138.3,24.88,209.4,13.08,36.15-6,69.85-17.84,104.45-29.34C989.49,25,1113-14.29,1200,52.47V0Z" opacity=".25"/><path fill="%23ffffff" d="M0,0V15.81C13,36.92,27.64,56.86,47.69,72.05,99.41,111.27,165,111,224.58,91.58c31.15-10.15,60.09-26.07,89.67-39.8,40.92-19,84.73-46,130.83-49.67,36.26-2.85,70.9,9.42,98.6,31.56,31.77,25.39,62.32,62,103.63,73,40.44,10.79,81.35-6.69,119.13-24.28s75.16-39,116.92-43.05c59.73-5.85,113.28,22.88,168.9,38.84,30.2,8.66,59,6.17,87.09-7.5,22.43-10.89,48-26.93,60.65-49.24V0Z" opacity=".5"/><path fill="%23ffffff" d="M0,0V5.63C149.93,59,314.09,71.32,475.83,42.57c43-7.64,84.23-20.12,127.61-26.46,59-8.63,112.48,12.24,165.56,35.4C827.93,77.22,886,95.24,951.2,90c86.53-7,172.46-45.71,248.8-84.81V0Z"/></svg>');
        }

        h2 {
            font-size: 2.5rem;
            margin-bottom: 40px;
            color: var(--primary);
            position: relative;
            display: inline-block;
            font-weight: 700;
        }

        h2::after {
            content: '';
            position: absolute;
            bottom: -15px;
            left: 0;
            width: 80px;
            height: 5px;
            background: var(--accent);
            border-radius: 3px;
        }

        h3 {
            font-size: 1.8rem;
            margin: 40px 0 20px;
            color: var(--dark);
            font-weight: 600;
        }

        h4 {
            font-size: 1.4rem;
            margin: 30px 0 15px;
            color: var(--primary);
            font-weight: 600;
        }

        p {
            margin-bottom: 25px;
            font-size: 1.15rem;
            line-height: 1.8;
            color: var(--dark);
        }

        .highlight {
            background-color: rgba(24, 119, 242, 0.1);
            padding: 3px 8px;
            border-radius: 4px;
            font-weight: 500;
            color: var(--primary);
        }

        ul, ol {
            margin-bottom: 25px;
            padding-left: 35px;
        }

        li {
            margin-bottom: 10px;
            font-size: 1.15rem;
            line-height: 1.7;
            color: var(--dark);
        }

        /* Table Styles */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 40px 0;
            box-shadow: var(--shadow);
            border-radius: var(--radius);
            overflow: hidden;
            animation: fadeInUp 1s ease;
            background: white;
        }

        th, td {
            padding: 18px;
            text-align: left;
            border-bottom: 1px solid rgba(0, 0, 0, 0.05);
        }

        th {
            background-color: var(--primary);
            color: white;
            font-weight: 500;
            font-size: 1.1rem;
        }

        tr:nth-child(even) {
            background-color: rgba(24, 119, 242, 0.03);
        }

        tr:hover {
            background-color: rgba(24, 119, 242, 0.08);
        }

        /* Card Styles */
        .card-container {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            margin: 50px 0;
        }

        .card {
            flex: 1 1 350px;
            background: white;
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            overflow: hidden;
            transition: var(--transition);
            position: relative;
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .card:hover {
            transform: translateY(-10px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.15);
        }

        .card-header {
            background: var(--primary);
            color: white;
            padding: 20px;
            font-weight: 600;
            font-size: 1.2rem;
        }

        .card-body {
            padding: 25px;
        }

        .card-footer {
            padding: 18px 25px;
            background: rgba(0, 0, 0, 0.02);
            border-top: 1px solid rgba(0, 0, 0, 0.05);
            font-size: 0.9rem;
            color: var(--secondary);
        }

        /* Tech Feature */
        .tech-feature {
            background: white;
            border-radius: var(--radius);
            padding: 30px;
            margin: 40px 0;
            box-shadow: var(--shadow);
            border-left: 5px solid var(--accent);
            transition: var(--transition);
            position: relative;
            overflow: hidden;
        }

        .tech-feature::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 5px;
            height: 100%;
            background: var(--accent);
        }

        .tech-feature:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
        }

        .tech-feature h4 {
            margin-top: 0;
            color: var(--primary);
        }

        /* Diagram */
        .diagram {
            background: white;
            border-radius: var(--radius);
            padding: 25px;
            margin: 40px 0;
            box-shadow: var(--shadow);
            text-align: center;
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .diagram img {
            max-width: 100%;
            height: auto;
            border-radius: var(--radius);
        }

        /* Footer */
        footer {
            background: var(--dark);
            color: white;
            padding: 60px 0 40px;
            text-align: center;
            position: relative;
            clip-path: polygon(0 20px, 100% 0, 100% 100%, 0 100%);
            margin-top: 60px;
        }

        footer::before {
            content: '';
            position: absolute;
            top: -60px;
            left: 0;
            width: 100%;
            height: 60px;
            background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1200 120" preserveAspectRatio="none"><path fill="%231C1E21" d="M0,0V46.29c47.79,22.2,103.59,32.17,158,28,70.36-5.37,136.33-33.31,206.8-37.5C438.64,32.43,512.34,53.67,583,72.05c69.27,18,138.3,24.88,209.4,13.08,36.15-6,69.85-17.84,104.45-29.34C989.49,25,1113-14.29,1200,52.47V0Z" opacity=".25"/><path fill="%231C1E21" d="M0,0V15.81C13,36.92,27.64,56.86,47.69,72.05,99.41,111.27,165,111,224.58,91.58c31.15-10.15,60.09-26.07,89.67-39.8,40.92-19,84.73-46,130.83-49.67,36.26-2.85,70.9,9.42,98.6,31.56,31.77,25.39,62.32,62,103.63,73,40.44,10.79,81.35-6.69,119.13-24.28s75.16-39,116.92-43.05c59.73-5.85,113.28,22.88,168.9,38.84,30.2,8.66,59,6.17,87.09-7.5,22.43-10.89,48-26.93,60.65-49.24V0Z" opacity=".5"/><path fill="%231C1E21" d="M0,0V5.63C149.93,59,314.09,71.32,475.83,42.57c43-7.64,84.23-20.12,127.61-26.46,59-8.63,112.48,12.24,165.56,35.4C827.93,77.22,886,95.24,951.2,90c86.53-7,172.46-45.71,248.8-84.81V0Z"/></svg>');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
        }

        .footer-content {
            max-width: 800px;
            margin: 0 auto;
            position: relative;
            z-index: 2;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            margin: 30px 0;
            flex-wrap: wrap;
        }

        .footer-links a {
            color: white;
            margin: 0 20px 15px;
            text-decoration: none;
            transition: var(--transition);
            font-size: 1rem;
        }

        .footer-links a:hover {
            color: var(--accent);
        }

        .copyright {
            opacity: 0.7;
            font-size: 0.95rem;
            margin-top: 30px;
        }

        /* Animations */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        @keyframes fadeInDown {
            from {
                opacity: 0;
                transform: translateY(-30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes slideInLeft {
            from {
                opacity: 0;
                transform: translateX(-50px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        @keyframes slideInRight {
            from {
                opacity: 0;
                transform: translateX(50px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        @keyframes bounce {
            0%, 20%, 50%, 80%, 100% {
                transform: translateY(0) translateX(-50%);
            }
            40% {
                transform: translateY(-20px) translateX(-50%);
            }
            60% {
                transform: translateY(-10px) translateX(-50%);
            }
        }

        @keyframes float {
            0%, 100% {
                transform: translate(0, 0) rotate(0deg);
            }
            25% {
                transform: translate(20px, 20px) rotate(5deg);
            }
            50% {
                transform: translate(0, 40px) rotate(0deg);
            }
            75% {
                transform: translate(-20px, 20px) rotate(-5deg);
            }
        }

        /* Responsive Styles */
        @media (max-width: 992px) {
            h1 {
                font-size: 2.8rem;
            }
            
            h2 {
                font-size: 2.2rem;
            }
            
            section {
                padding: 60px 0;
            }
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2.4rem;
            }

            h2 {
                font-size: 2rem;
            }
            
            h3 {
                font-size: 1.6rem;
            }

            nav {
                padding: 15px 0;
                justify-content: flex-start;
                overflow-x: auto;
                white-space: nowrap;
                -webkit-overflow-scrolling: touch;
            }
            
            nav a {
                margin: 0 12px;
                font-size: 0.95rem;
                padding: 6px 10px;
            }

            section {
                padding: 50px 0;
            }
            
            section::before {
                top: -40px;
                height: 40px;
            }
            
            .card {
                flex: 1 1 100%;
            }
        }

        @media (max-width: 576px) {
            h1 {
                font-size: 2rem;
            }
            
            h2 {
                font-size: 1.8rem;
            }
            
            header {
                padding: 80px 0 100px;
            }
            
            .subtitle {
                font-size: 1.1rem;
            }
            
            p, li {
                font-size: 1.05rem;
            }
            
            th, td {
                padding: 12px;
                font-size: 0.95rem;
            }
        }

        /* Floating Elements */
        .floating-llama {
            position: fixed;
            width: 120px;
            height: 120px;
            background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="%231877F2"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zm-5.5-2.5l7.51-3.49L17.5 6.5 9.99 9.99 6.5 17.5zm5.5-6.6c.61 0 1.1.49 1.1 1.1s-.49 1.1-1.1 1.1-1.1-.49-1.1-1.1.49-1.1 1.1-1.1z"/></svg>');
            background-size: contain;
            background-repeat: no-repeat;
            opacity: 0.08;
            z-index: -1;
            animation: float 15s infinite ease-in-out;
        }

        /* Back to Top Button */
        .back-to-top {
            position: fixed;
            bottom: 40px;
            right: 40px;
            width: 60px;
            height: 60px;
            background: var(--primary);
            color: white;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            opacity: 0;
            visibility: hidden;
            transition: var(--transition);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
            z-index: 99;
            font-size: 1.5rem;
        }

        .back-to-top.active {
            opacity: 1;
            visibility: visible;
        }

        .back-to-top:hover {
            background: var(--accent);
            transform: translateY(-5px) scale(1.05);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
        }

        /* Math Formula */
        .math-formula {
            background: rgba(24, 119, 242, 0.05);
            padding: 20px;
            border-radius: var(--radius);
            margin: 25px 0;
            font-family: 'Courier New', Courier, monospace;
            overflow-x: auto;
            border-left: 3px solid var(--primary);
            font-size: 1.1rem;
        }

        /* Code Block */
        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: var(--radius);
            margin: 25px 0;
            font-family: 'Courier New', Courier, monospace;
            overflow-x: auto;
            font-size: 1rem;
            line-height: 1.6;
        }

        .code-block .keyword {
            color: #c678dd;
        }
        
        .code-block .function {
            color: #61afef;
        }
        
        .code-block .string {
            color: #98c379;
        }
        
        .code-block .comment {
            color: #5c6370;
            font-style: italic;
        }
        
        .code-block .number {
            color: #d19a66;
        }

        /* Progress Bar */
        .progress-container {
            width: 100%;
            background-color: #f1f1f1;
            border-radius: var(--radius);
            margin: 35px 0;
            overflow: hidden;
            height: 10px;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, var(--primary), var(--accent));
            border-radius: var(--radius);
            width: 0;
            transition: width 1.5s ease;
        }

        /* Tooltip */
        .tooltip {
            position: relative;
            display: inline-block;
            border-bottom: 1px dotted var(--primary);
            cursor: help;
        }

        .tooltip .tooltiptext {
            visibility: hidden;
            width: 220px;
            background-color: var(--dark);
            color: #fff;
            text-align: center;
            border-radius: var(--radius);
            padding: 12px;
            position: absolute;
            z-index: 1;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            opacity: 0;
            transition: opacity 0.3s;
            font-size: 0.95rem;
            line-height: 1.5;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
        }

        .tooltip .tooltiptext::after {
            content: "";
            position: absolute;
            top: 100%;
            left: 50%;
            margin-left: -5px;
            border-width: 5px;
            border-style: solid;
            border-color: var(--dark) transparent transparent transparent;
        }

        .tooltip:hover .tooltiptext {
            visibility: visible;
            opacity: 1;
        }

        /* Timeline */
        .timeline {
            position: relative;
            max-width: 1000px;
            margin: 50px auto;
            padding: 0 20px;
        }

        .timeline::before {
            content: '';
            position: absolute;
            top: 0;
            bottom: 0;
            left: 50%;
            width: 4px;
            background: var(--primary);
            transform: translateX(-50%);
        }

        .timeline-item {
            position: relative;
            margin-bottom: 50px;
            width: 50%;
            padding: 20px;
            animation: fadeIn 1s ease;
        }

        .timeline-item:nth-child(odd) {
            left: 0;
        }

        .timeline-item:nth-child(even) {
            left: 50%;
        }

        .timeline-content {
            background: white;
            padding: 30px;
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            position: relative;
        }

        .timeline-content::after {
            content: '';
            position: absolute;
            top: 20px;
            width: 20px;
            height: 20px;
            background: white;
            transform: rotate(45deg);
            box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.1);
        }

        .timeline-item:nth-child(odd) .timeline-content::after {
            right: -10px;
        }

        .timeline-item:nth-child(even) .timeline-content::after {
            left: -10px;
        }

        .timeline-date {
            display: block;
            font-weight: 600;
            color: var(--accent);
            margin-bottom: 15px;
            font-size: 1.1rem;
        }

        .timeline-item:nth-child(odd) {
            animation: slideInLeft 1s ease;
        }

        .timeline-item:nth-child(even) {
            animation: slideInRight 1s ease;
        }

        /* Model Comparison */
        .model-comparison {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            margin: 50px 0;
        }

        .model-card {
            flex: 1 1 300px;
            background: white;
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            overflow: hidden;
            transition: var(--transition);
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .model-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.15);
        }

        .model-header {
            padding: 20px;
            background: var(--primary);
            color: white;
            font-weight: 600;
            font-size: 1.2rem;
            text-align: center;
        }

        .model-body {
            padding: 25px;
        }

        .model-specs {
            margin-bottom: 15px;
        }

        .model-specs h5 {
            font-size: 1rem;
            color: var(--secondary);
            margin-bottom: 10px;
        }

        .model-specs ul {
            padding-left: 20px;
            margin-bottom: 0;
        }

        .model-specs li {
            font-size: 0.95rem;
            margin-bottom: 8px;
            color: var(--dark);
        }

        /* Feature Grid */
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 30px;
            margin: 50px 0;
        }

        .feature-item {
            background: white;
            border-radius: var(--radius);
            padding: 30px;
            box-shadow: var(--shadow);
            transition: var(--transition);
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .feature-item:hover {
            transform: translateY(-10px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.15);
        }

        .feature-icon {
            font-size: 2.5rem;
            color: var(--primary);
            margin-bottom: 20px;
            text-align: center;
        }

        .feature-title {
            font-size: 1.3rem;
            color: var(--primary);
            margin-bottom: 15px;
            font-weight: 600;
            text-align: center;
        }
    </style>
</head>
<body>
    <!-- Floating Background Elements -->
    <div class="floating-llama" style="top: 20%; left: 10%;"></div>
    <div class="floating-llama" style="top: 60%; left: 80%; animation-delay: 2s;"></div>
    <div class="floating-llama" style="top: 30%; left: 70%; animation-delay: 4s;"></div>
    <div class="floating-llama" style="top: 70%; left: 20%; animation-delay: 6s;"></div>

    <!-- Header -->
    <header>
        <div class="container">
            <h1>Llama 2 全面学习指南</h1>
            <p class="subtitle">Meta开源大语言模型的技术解析与实践应用</p>
        </div>
        <div class="scroll-down" onclick="scrollToContent()">↓</div>
    </header>

    <!-- Navigation -->
    <div class="nav-container" id="navbar">
        <nav>
            <a href="#overview" class="active">概述</a>
            <a href="#architecture">核心架构</a>
            <a href="#features">关键特性</a>
            <a href="#training">训练方法</a>
            <a href="#performance">性能评估</a>
            <a href="#application">应用实践</a>
        </nav>
    </div>

    <!-- Main Content -->
    <div class="container">
        <!-- Overview Section -->
        <section id="overview">
            <h2>Llama 2 概述</h2>
            <p>Llama 2是Meta于2023年7月推出的开源大型语言模型系列，作为Llama 1的升级版本，它在模型性能、安全性和实用性方面都有显著提升。Llama 2系列包括7B、13B和70B三种参数规模的模型，以及专门优化的对话模型Llama 2-Chat。</p>
            
            <div class="tech-feature">
                <h4>Llama 2的主要突破</h4>
                <ul>
                    <li><strong>完全开源</strong>：包括模型权重和代码，支持商业用途</li>
                    <li><strong>性能提升</strong>：在多项基准测试中超越Llama 1和GPT-3</li>
                    <li><strong>安全增强</strong>：通过RLHF技术减少有害输出</li>
                    <li><strong>对话优化</strong>：专门的对话模型Llama 2-Chat</li>
                </ul>
            </div>
            
            <h3>Llama 2系列模型</h3>
            
            <div class="model-comparison">
                <div class="model-card">
                    <div class="model-header">Llama 2-7B</div>
                    <div class="model-body">
                        <div class="model-specs">
                            <h5>规格参数</h5>
                            <ul>
                                <li>70亿参数</li>
                                <li>32层Transformer</li>
                                <li>32个注意力头</li>
                                <li>上下文长度4096</li>
                            </ul>
                        </div>
                        <div class="model-specs">
                            <h5>适用场景</h5>
                            <ul>
                                <li>边缘设备部署</li>
                                <li>研究实验</li>
                                <li>轻量级应用</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="model-card">
                    <div class="model-header">Llama 2-13B</div>
                    <div class="model-body">
                        <div class="model-specs">
                            <h5>规格参数</h5>
                            <ul>
                                <li>130亿参数</li>
                                <li>40层Transformer</li>
                                <li>40个注意力头</li>
                                <li>上下文长度4096</li>
                            </ul>
                        </div>
                        <div class="model-specs">
                            <h5>适用场景</h5>
                            <ul>
                                <li>中小型企业应用</li>
                                <li>学术研究</li>
                                <li>API服务</li>
                            </ul>
                        </div>
                    </div>
                </div>
                
                <div class="model-card">
                    <div class="model-header">Llama 2-70B</div>
                    <div class="model-body">
                        <div class="model-specs">
                            <h5>规格参数</h5>
                            <ul>
                                <li>700亿参数</li>
                                <li>80层Transformer</li>
                                <li>64个注意力头</li>
                                <li>上下文长度4096</li>
                            </ul>
                        </div>
                        <div class="model-specs">
                            <h5>适用场景</h5>
                            <ul>
                                <li>企业级应用</li>
                                <li>云服务</li>
                                <li>尖端研究</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            
            <h3>历史意义</h3>
            <p>Llama 2的发布标志着开源大语言模型进入了一个新阶段：</p>
            <ul>
                <li><strong>商业友好</strong>：首个完全开源支持商业使用的主流大模型</li>
                <li><strong>性能标杆</strong>：70B版本在多项测试中接近GPT-4水平</li>
                <li><strong>生态繁荣</strong>：催生了大量基于Llama 2的衍生模型和应用</li>
            </ul>
        </section>

        <!-- Architecture Section -->
        <section id="architecture">
            <h2>核心架构与技术特点</h2>
            
            <p>Llama 2在Llama 1架构基础上进行了多项创新，这些改进共同提升了模型的性能和效率。</p>
            
            <h3>Transformer架构优化</h3>
            <p>Llama 2保留了Llama 1的纯解码器Transformer架构，但进行了以下关键改进：</p>
            
            <div class="feature-grid">
                <div class="feature-item">
                    <div class="feature-icon">🧠</div>
                    <h4 class="feature-title">分组查询注意力(GQA)</h4>
                    <p>Llama 2引入了分组查询注意力(Grouped Query Attention)机制，在保持模型性能的同时显著降低了KV缓存的内存占用。</p>
                </div>
                
                <div class="feature-item">
                    <div class="feature-icon">⚡</div>
                    <h4 class="feature-title">RMSNorm预归一化</h4>
                    <p>延续Llama 1的RMSNorm层归一化，但在更大规模的模型上验证了其有效性，提高了训练稳定性。</p>
                </div>
                
                <div class="feature-item">
                    <div class="feature-icon">🔄</div>
                    <h4 class="feature-title">旋转位置编码(RoPE)</h4>
                    <p>优化了RoPE实现，支持更长的上下文长度(4096 tokens)，提升了长文本处理能力。</p>
                </div>
            </div>
            
            <h3>分组查询注意力(GQA)</h3>
            <p>GQA是Llama 2最重要的创新之一，它通过共享key和value的投影来减少内存占用：</p>
            
            <div class="tech-feature">
                <h4>GQA工作原理</h4>
                <ul>
                    <li>将查询头(Q)分组，每组共享相同的key和value投影</li>
                    <li>在70B模型中，8个查询头共享1个key-value头</li>
                    <li>KV缓存大小减少为原来的1/8，显著降低推理内存需求</li>
                </ul>
            </div>
            
            <div class="diagram">
                <p><em>[GQA架构示意图]</em></p>
                <div style="background:#eee; height:300px; display:flex; justify-content:center; align-items:center; border-radius:var(--radius);">
                    <p>分组查询注意力(GQA)机制示意图</p>
                </div>
            </div>
            
            <h3>SwiGLU激活函数</h3>
            <p>Llama 2继续使用SwiGLU激活函数，其数学表达式为：</p>
            
            <div class="math-formula">
                SwiGLU(x) = (swish(W₁x)) ⊙ (W₂x)<br>
                其中swish(x) = x · σ(x)，σ表示sigmoid函数，⊙表示逐元素乘法
            </div>
            
            <p>与标准FFN相比，SwiGLU需要三个权重矩阵(W₁、W₂和W₃)，但能提供更强的表达能力。</p>
            
            <h3>架构对比</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>架构组件</th>
                        <th>Llama 1</th>
                        <th>Llama 2</th>
                        <th>改进效果</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>注意力机制</td>
                        <td>标准多头注意力</td>
                        <td>分组查询注意力(GQA)</td>
                        <td>内存占用↓，推理效率↑</td>
                    </tr>
                    <tr>
                        <td>位置编码</td>
                        <td>RoPE</td>
                        <td>优化RoPE</td>
                        <td>支持更长上下文</td>
                    </tr>
                    <tr>
                        <td>归一化</td>
                        <td>RMSNorm</td>
                        <td>RMSNorm</td>
                        <td>训练稳定性↑</td>
                    </tr>
                    <tr>
                        <td>激活函数</td>
                        <td>SwiGLU</td>
                        <td>SwiGLU</td>
                        <td>表达能力↑</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Features Section -->
        <section id="features">
            <h2>关键特性与创新</h2>
            
            <p>Llama 2在Llama 1基础上引入了多项创新特性，使其成为开源大模型的新标杆。</p>
            
            <h3>对话模型优化</h3>
            <p>Llama 2-Chat是专门针对对话场景优化的版本，通过以下技术提升对话质量：</p>
            
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-content">
                        <span class="timeline-date">监督微调(SFT)</span>
                        <p>使用人工标注的高质量对话数据对基础模型进行微调，建立初始对话能力。</p>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-content">
                        <span class="timeline-date">奖励模型训练</span>
                        <p>训练专门的奖励模型(RM)来评估对话回复的质量，作为RLHF的基础。</p>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-content">
                        <span class="timeline-date">RLHF优化</span>
                        <p>采用强化学习从人类反馈(RLHF)进一步优化模型，包括PPO和拒绝采样。</p>
                    </div>
                </div>
            </div>
            
            <h3>安全性与对齐</h3>
            <p>Llama 2在安全性方面做了大量工作：</p>
            
            <div class="card-container">
                <div class="card">
                    <div class="card-header">安全微调</div>
                    <div class="card-body">
                        <ul>
                            <li>使用安全相关数据微调模型</li>
                            <li>减少有害、偏见内容生成</li>
                            <li>提高模型对敏感话题的处理能力</li>
                        </ul>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-header">红队测试</div>
                    <div class="card-body">
                        <ul>
                            <li>组织专家团队进行对抗测试</li>
                            <li>发现并修复潜在安全问题</li>
                            <li>持续迭代改进模型安全性</li>
                        </ul>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-header">透明度</div>
                    <div class="card-body">
                        <ul>
                            <li>公开模型能力和限制</li>
                            <li>提供负责任使用指南</li>
                            <li>鼓励社区安全研究</li>
                        </ul>
                    </div>
                </div>
            </div>
            
            <h3>开源策略</h3>
            <p>Llama 2的开源策略是其成功的关键因素：</p>
            <ul>
                <li><strong>商业友好许可</strong>：允许免费商用，极大降低了企业使用门槛</li>
                <li><strong>完整代码库</span>：提供训练、推理和微调的全套工具</li>
                <li><strong>社区支持</strong>：Meta积极维护并与开发者社区互动</li>
            </ul>
        </section>

        <!-- Training Section -->
        <section id="training">
            <h2>训练方法与数据处理</h2>
            
            <p>Llama 2的训练流程体现了Meta在大模型训练方面的专业经验，通过精心设计的数据策略和训练技术实现了高效学习。</p>
            
            <h3>训练数据构成</h3>
            <p>Llama 2使用比Llama 1更丰富的数据源，包括：</p>
            
            <div class="progress-container">
                <div class="progress-bar" id="training-data-progress"></div>
            </div>
            
            <ul>
                <li><strong>公开网页数据</strong>：经过严格过滤的CommonCrawl和C4数据</li>
                <li><strong>技术内容</strong>：GitHub代码、技术文档和论文</li>
                <li><strong>百科知识</strong>：多语言Wikipedia内容</li>
                <li><strong>书籍数据</strong>：公开领域的书籍文本</li>
                <li><strong>对话数据</strong>：用于微调Llama 2-Chat</li>
            </ul>
            
            <h3>预训练流程</h3>
            <p>Llama 2的预训练采用以下关键技术：</p>
            
            <div class="tech-feature">
                <h4>高效训练策略</h4>
                <ul>
                    <li><strong>优化器</strong>：AdamW，β₁=0.9，β₂=0.95，ε=10⁻⁵</li>
                    <li><strong>学习率</strong>：余弦衰减，峰值3×10⁻⁴，最终降至峰值10%</li>
                    <li><strong>批量大小</strong>：动态调整，最大400万token</li>
                    <li><strong>权重衰减</strong>：0.1，梯度裁剪阈值1.0</li>
                </ul>
            </div>
            
            <h3>RLHF流程</h3>
            <p>Llama 2-Chat通过强化学习从人类反馈(RLHF)进行优化：</p>
            
            <div class="card-container">
                <div class="card">
                    <div class="card-header">初始监督微调</div>
                    <div class="card-body">
                        <p>使用人工标注的高质量对话数据对基础模型进行微调，建立初步对话能力。</p>
                        <div class="code-block">
                            # 伪代码示例 - 监督微调
                            <span class="keyword">for</span> batch <span class="keyword">in</span> dialog_dataset:
                                inputs = tokenizer(batch[<span class="string">"prompt"</span>])
                                labels = tokenizer(batch[<span class="string">"response"</span>])
                                outputs = model(**inputs, labels=labels)
                                loss = outputs.loss
                                loss.backward()
                                optimizer.step()
                        </div>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-header">奖励模型训练</div>
                    <div class="card-body">
                        <p>训练奖励模型(RM)评估回复质量，基于人工对多个回复的排序。</p>
                        <div class="code-block">
                            # 伪代码示例 - 奖励模型
                            <span class="keyword">class</span> <span class="function">RewardModel</span>(nn.Module):
                                <span class="keyword">def</span> <span class="function">__init__</span>(self, base_model):
                                    self.model = base_model
                                    self.head = nn.Linear(hidden_size, <span class="number">1</span>)
                                
                                <span class="keyword">def</span> <span class="function">forward</span>(self, input_ids):
                                    outputs = self.model(input_ids)
                                    <span class="keyword">return</span> self.head(outputs.last_hidden_state[:, -<span class="number">1</span>])
                        </div>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-header">PPO优化</div>
                    <div class="card-body">
                        <p>使用近端策略优化(PPO)算法根据奖励模型反馈优化对话策略。</p>
                        <div class="code-block">
                            # 伪代码示例 - PPO训练
                            <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:
                                responses = generate_responses(prompts)
                                rewards = reward_model(responses)
                                loss = ppo_loss(policy, old_policy, rewards)
                                loss.backward()
                                optimizer.step()
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Performance Section -->
        <section id="performance">
            <h2>性能评估与对比分析</h2>
            
            <p>Llama 2在多项基准测试中展现了卓越的性能，特别是在开源模型中树立了新标杆。</p>
            
            <h3>基准测试表现</h3>
            <p>Llama 2在以下关键测试集上的表现：</p>
            
            <table>
                <thead>
                    <tr>
                        <th>测试集</th>
                        <th>Llama 2-7B</th>
                        <th>Llama 2-13B</th>
                        <th>Llama 2-70B</th>
                        <th>GPT-3.5</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>MMLU(5-shot)</td>
                        <td>45.3</td>
                        <td>54.8</td>
                        <td>68.9</td>
                        <td>70.0</td>
                    </tr>
                    <tr>
                        <td>GSM8K(8-shot)</td>
                        <td>14.6</td>
                        <td>28.7</td>
                        <td>56.2</td>
                        <td>57.1</td>
                    </tr>
                    <tr>
                        <td>HumanEval(0-shot)</td>
                        <td>12.8</td>
                        <td>20.4</td>
                        <td>32.3</td>
                        <td>33.1</td>
                    </tr>
                    <tr>
                        <td>ARC-Challenge(25-shot)</td>
                        <td>47.6</td>
                        <td>55.3</td>
                        <td>67.3</td>
                        <td>68.1</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>对话模型评估</h3>
            <p>Llama 2-Chat在对话任务上的表现：</p>
            
            <div class="tech-feature">
                <h4>人工评估结果</h4>
                <p>Meta组织了大规模人工评估，比较Llama 2-Chat与其他模型的对话质量：</p>
                <ul>
                    <li>在帮助性和安全性上优于大多数开源模型</li>
                    <li>70B版本接近GPT-3.5-turbo水平</li>
                    <li>在非英语语言上表现优于同类开源模型</li>
                </ul>
            </div>
            
            <h3>效率评估</h3>
            <p>Llama 2在计算效率方面的优势：</p>
            <ul>
                <li><strong>GQA带来的内存节省</strong>：70B模型的KV缓存减少为原来的1/8</li>
                <li><strong>推理速度</strong>：比同等规模的传统架构快1.5-2倍</li>
                <li><strong>训练效率</strong>：相同计算预算下获得更高性能</li>
            </ul>
        </section>

        <!-- Application Section -->
        <section id="application">
            <h2>应用实践与部署指南</h2>
            
            <p>Llama 2的开源特性使其在各种应用场景中都能灵活部署和使用。</p>
            
            <h3>典型应用场景</h3>
            
            <div class="feature-grid">
                <div class="feature-item">
                    <div class="feature-icon">💬</div>
                    <h4 class="feature-title">对话系统</h4>
                    <p>使用Llama 2-Chat构建智能客服、个人助手等对话应用，支持多轮上下文理解。</p>
                </div>
                
                <div class="feature-item">
                    <div class="feature-icon">📝</div>
                    <h4 class="feature-title">内容生成</h4>
                    <p>用于文章写作、创意生成、代码补全等场景，提高内容创作效率。</p>
                </div>
                
                <div class="feature-item">
                    <div class="feature-icon">🔍</div>
                    <h4 class="feature-title">知识问答</h4>
                    <p>构建闭卷知识问答系统，无需外部检索即可回答专业问题。</p>
                </div>
                
                <div class="feature-item">
                    <div class="feature-icon">🧪</div>
                    <h4 class="feature-title">研究平台</h4>
                    <p>作为NLP研究的基线模型，支持各种改进算法的实验验证。</p>
                </div>
            </div>
            
            <h3>部署方案</h3>
            <p>Llama 2可以通过多种方式部署：</p>
            
            <div class="tech-feature">
                <h4>Hugging Face Transformers</h4>
                <p>使用Hugging Face生态系统加载和运行Llama 2：</p>
                <div class="code-block">
                    <span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM
                    
                    tokenizer = AutoTokenizer.from_pretrained(<span class="string">"meta-llama/Llama-2-7b-chat-hf"</span>)
                    model = AutoModelForCausalLM.from_pretrained(<span class="string">"meta-llama/Llama-2-7b-chat-hf"</span>)
                    
                    inputs = tokenizer(<span class="string">"Hello, how are you?"</span>, return_tensors=<span class="string">"pt"</span>)
                    outputs = model.generate(**inputs, max_length=<span class="number">100</span>)
                    print(tokenizer.decode(outputs[<span class="number">0</span>]))
                </div>
            </div>
            
            <div class="tech-feature">
                <h4>量化部署</h4>
                <p>使用量化技术减少模型资源需求：</p>
                <ul>
                    <li><strong>8-bit量化</strong>：几乎无损，内存减少50%</li>
                    <li><strong>4-bit量化</strong>：轻微质量损失，内存减少75%</li>
                    <li><strong>GPTQ</strong>：后训练量化，保持较高精度</li>
                </ul>
                <div class="code-block">
                    <span class="comment"># 使用bitsandbytes进行8-bit量化</span>
                    <span class="keyword">from</span> transformers <span class="keyword">import</span> BitsAndBytesConfig
                    
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=<span class="keyword">True</span>,
                        llm_int8_threshold=<span class="number">6.0</span>
                    )
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        <span class="string">"meta-llama/Llama-2-7b"</span>,
                        quantization_config=quantization_config
                    )
                </div>
            </div>
            
            <h3>微调指南</h3>
            <p>针对特定任务微调Llama 2的基本流程：</p>
            
            <div class="card-container">
                <div class="card">
                    <div class="card-header">1. 准备数据</div>
                    <div class="card-body">
                        <ul>
                            <li>收集领域特定数据</li>
                            <li>格式化为模型输入</li>
                            <li>划分训练/验证集</li>
                        </ul>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-header">2. 选择方法</div>
                    <div class="card-body">
                        <ul>
                            <li>全参数微调</li>
                            <li>LoRA低秩适配</li>
                            <li>Prompt tuning</li>
                        </ul>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-header">3. 训练配置</div>
                    <div class="card-body">
                        <ul>
                            <li>学习率: 1e-5到5e-5</li>
                            <li>批量大小: 根据GPU调整</li>
                            <li>训练轮次: 3-10</li>
                        </ul>
                    </div>
                </div>
            </div>
            
            <div class="tech-feature">
                <h4>LoRA微调示例</h4>
                <div class="code-block">
                    <span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model
                    
                    <span class="comment"># 配置LoRA</span>
                    lora_config = LoraConfig(
                        r=<span class="number">8</span>,
                        lora_alpha=<span class="number">16</span>,
                        target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],
                        lora_dropout=<span class="number">0.1</span>,
                        bias=<span class="string">"none"</span>
                    )
                    
                    <span class="comment"># 应用LoRA</span>
                    model = get_peft_model(model, lora_config)
                    model.print_trainable_parameters()
                    
                    <span class="comment"># 训练循环</span>
                    trainer = Trainer(
                        model=model,
                        args=training_args,
                        train_dataset=train_dataset,
                        eval_dataset=eval_dataset
                    )
                    trainer.train()
                </div>
            </div>
            
            <h3>学习资源</h3>
            <p>要进一步学习Llama 2及相关技术，可以参考以下资源：</p>
            
            <table>
                <thead>
                    <tr>
                        <th>资源类型</th>
                        <th>链接/名称</th>
                        <th>描述</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>官方文档</td>
                        <td>Llama 2 Research Paper</td>
                        <td>Meta官方技术报告</td>
                    </tr>
                    <tr>
                        <td>代码仓库</td>
                        <td>GitHub - facebookresearch/llama</td>
                        <td>官方实现代码</td>
                    </tr>
                    <tr>
                        <td>模型权重</td>
                        <td>Hugging Face Model Hub</td>
                        <td>预训练模型下载</td>
                    </tr>
                    <tr>
                        <td>教程</td>
                        <td>Llama 2 Fine-tuning Guide</td>
                        <td>微调实践指南</td>
                    </tr>
                </tbody>
            </table>
        </section>
    </div>

    <!-- Footer -->
    <footer>
        <div class="footer-content">
            <h3>Llama 2 全面学习指南</h3>
            <p>Meta开源大语言模型的技术解析与实践应用</p>
            
            <div class="footer-links">
                <a href="#overview">概述</a>
                <a href="#architecture">核心架构</a>
                <a href="#features">关键特性</a>
                <a href="#training">训练方法</a>
                <a href="#performance">性能评估</a>
                <a href="#application">应用实践</a>
            </div>
            
            <p class="copyright">© 2025 AI技术学习文档 | 基于Meta Llama 2技术报告整理</p>
        </div>
    </footer>

    <!-- Back to Top Button -->
    <div class="back-to-top" onclick="scrollToTop()">↑</div>

    <!-- JavaScript -->
    <script>
        // Scroll to content function
        function scrollToContent() {
            document.querySelector('#overview').scrollIntoView({
                behavior: 'smooth'
            });
        }

        // Back to top button
        window.addEventListener('scroll', function() {
            var backToTop = document.querySelector('.back-to-top');
            var navbar = document.getElementById('navbar');
            
            if (window.pageYOffset > 300) {
                backToTop.classList.add('active');
                navbar.classList.add('scrolled');
            } else {
                backToTop.classList.remove('active');
                navbar.classList.remove('scrolled');
            }
        });

        function scrollToTop() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        }

        // Navigation active state
        const sections = document.querySelectorAll('section');
        const navLinks = document.querySelectorAll('nav a');

        window.addEventListener('scroll', function() {
            let current = '';
            
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                
                if (pageYOffset >= sectionTop - 300) {
                    current = section.getAttribute('id');
                }
            });
            
            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        });

        // Animate elements when they come into view
        const animateOnScroll = function() {
            const elements = document.querySelectorAll('.tech-feature, table, .card, .diagram, .feature-item, .timeline-item');
            
            elements.forEach(element => {
                const elementPosition = element.getBoundingClientRect().top;
                const windowHeight = window.innerHeight;
                
                if (elementPosition < windowHeight - 100) {
                    element.style.opacity = '1';
                    element.style.transform = 'translateY(0)';
                }
            });
        };

        // Animate progress bars
        function animateProgressBars() {
            const progressBars = document.querySelectorAll('.progress-bar');
            
            progressBars.forEach(bar => {
                let width = 0;
                const targetWidth = bar.id === 'training-data-progress' ? 92 : 85;
                const interval = setInterval(() => {
                    if (width >= targetWidth) {
                        clearInterval(interval);
                    } else {
                        width++;
                        bar.style.width = width + '%';
                    }
                }, 20);
            });
        }

        // Set initial state for animated elements
        document.querySelectorAll('.tech-feature, table, .card, .diagram, .feature-item, .timeline-item').forEach(element => {
            element.style.opacity = '0';
            element.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
            
            if (element.classList.contains('timeline-item:nth-child(odd)') || 
                element.classList.contains('feature-item')) {
                element.style.transform = 'translateX(-30px)';
            } else if (element.classList.contains('timeline-item:nth-child(even)')) {
                element.style.transform = 'translateX(30px)';
            } else {
                element.style.transform = 'translateY(30px)';
            }
        });

        // Add scroll event listener
        window.addEventListener('scroll', animateOnScroll);
        
        // Run animations on page load
        window.addEventListener('load', function() {
            animateOnScroll();
            animateProgressBars();
            
            // Smooth scroll for anchor links
            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                anchor.addEventListener('click', function(e) {
                    e.preventDefault();
                    
                    const targetId = this.getAttribute('href');
                    if (targetId === '#') return;
                    
                    const targetElement = document.querySelector(targetId);
                    if (targetElement) {
                        targetElement.scrollIntoView({
                            behavior: 'smooth'
                        });
                        
                        // Update URL without jumping
                        history.pushState(null, null, targetId);
                    }
                });
            });
        });
    </script>
    <!-- 添加返回主页链接 -->
    <a href="../../../" class="back-link">← 返回主页</a>
</body>
</html>