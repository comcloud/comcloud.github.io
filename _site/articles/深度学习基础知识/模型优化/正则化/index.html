<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>正则化</title>
  <meta name="description" content="">
  <link rel="canonical" href="https://comcloud.github.io/articles/深度学习基础知识/模型优化/正则化/">
  <link href="/assets/css/style.css" rel="stylesheet">
  <link href="/assets/css/temp-style.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i%7CNoto+Serif:400,400i,700,700i&display=swap" rel="stylesheet">
  
  <!-- Enhanced Prism.js for comprehensive syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet">
  
  <!-- MathJax for math formulas -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <!-- Mermaid for diagrams -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      themeVariables: {
        primaryColor: '#2d72d9',
        primaryTextColor: '#333',
        primaryBorderColor: '#2d72d9',
        lineColor: '#666'
      }
    });
  </script>
</head>
<body>
  <div id="page" class="site">
    <div class="inner">
      <header class="site-header">
        <p class="site-title"><a class="logo-text" href="/">成都犀牛</a></p>
        <nav class="site-navigation">
          <div class="site-navigation-wrap">
            <ul class="menu">
              <li class="menu-item"><a href="/">Home</a></li>
              <li class="menu-item"><a href="/personal/">Personal</a></li>
              <li class="menu-item"><a href="/articles/">Articles</a></li>
            </ul>
          </div>
        </nav>
      </header>
      
      <main class="main-content fadeInDown delay_075s">
        <article class="post article-content">
          <header class="post-header">
            <h1 class="post-title">正则化</h1>
            
            
            
          </header>
          <div class="post-content">
            <blockquote>
  <p>正则化（<code class="language-plaintext highlighter-rouge">Regularization</code>）是一种在机器学习中用来防止过拟合的技术。过拟合是指模型在训练数据上表现很好，但在新数据上的泛化能力较差。正则化通过<strong>在损失函数中增加额外的惩罚项</strong>，限制模型的复杂度，从而提升模型的泛化能力。</p>
</blockquote>

<ol>
  <li><strong>L1 正则化</strong>：也称为 Lasso 正则化，它通过在模型的损失函数中增加权重的 L1 范数（权重向量的绝对值之和）来实现正则化。<a href="https://so.csdn.net/so/search?q=L1 正则化&amp;spm=1001.2101.3001.7020">L1 正则化</a>倾向于产生稀疏权重矩阵，即将一些权重推向零，从而实现特征选择的效果。</li>
  <li><strong>L2 正则化</strong>：也称为 Ridge 正则化，它通过在模型的损失函数中增加权重的 L2 范数（权重向量的平方和）来实现正则化。L2 正则化会使权重值变得较小，但不会直接导致权重稀疏，因此不具有特征选择的作用，但可以有效地控制模型的复杂度。</li>
  <li><strong>Elastic Net 正则化</strong>：Elastic Net 是 L1 和 L2 正则化的组合，它在损失函数中同时使用 L1 和 L2 范数，可以综合两者的优点。</li>
  <li><strong>Dropout</strong>：Dropout 是一种特殊的正则化技术，通过在训练过程中随机地丢弃（将其权重置为零）网络中的部分神经元，以及它们的连接，来减少神经网络的复杂度。这样可以防止神经元之间的共适应性，从而减少过拟合。</li>
  <li><strong>早停（Early Stopping）</strong>：早停是一种简单而有效的正则化方法，它在训练过程中监视模型在验证集上的性能，一旦验证集上的性能开始下降，就停止训练。这样可以避免模型在训练集上过拟合。</li>
  <li><strong>数据增强（Data Augmentation）</strong>：数据增强是通过对训练数据进行变换来增加数据的多样性，从而减少过拟合的风险。例如，在图像分类任务中可以进行随机裁剪、旋转、翻转等操作来增加训练数据的数量和多样性。</li>
  <li><strong>权重衰减（Weight Decay）</strong>：权重衰减是一种通过在损失函数中增加权重的平方和或绝对值之和来实现正则化的技术。它等价于对权重参数进行 L2 正则化。</li>
</ol>

<h4 id="1-l1-正则化lasso-正则化">1. <strong>L1 正则化（Lasso 正则化）</strong></h4>
<ul>
  <li>
    <p><strong>原理</strong>：L1 正则化通过在损失函数中加入参数的<strong>绝对值</strong>之和来限制模型复杂度，形式为：
\(L(\theta) = \text{Loss}(\theta) + \lambda \sum_{i=1}^{n} |\theta_i|\)</p>

    <p>其中，( $\theta$ ) 是模型参数，( $\lambda$ ) 是正则化参数。</p>
  </li>
  <li><strong>作用</strong>：L1 正则化具有<strong>稀疏性</strong>，即它倾向于将一些参数的权重推向零。</li>
  <li><strong>应用</strong>：常用于特征选择，尤其是在特征数量非常大的情况下。</li>
</ul>

<blockquote>
  <p>L1 正则化能够<strong>自动进行特征选择</strong>，去掉不重要的特征。原因在于<strong>绝对值函数的导数在零点处不连续</strong>，这意味着在优化过程中，如果某个参数的绝对值小到一定程度，L1 正则化会强制它变为零，从而<strong>自动丢弃不重要的特征</strong>。</p>

  <p><strong>几何直观：</strong> L1 正则化在参数空间中的惩罚是一个<strong>菱形</strong>（在二维空间下），其顶点位于坐标轴上，这意味着最优解容易位于坐标轴上（即某些参数值为零）。因此，L1 正则化倾向于将部分参数推向零，从而实现特征选择。</p>
</blockquote>

<p><img src="/Users/rayss/Public/work/AI求职/study/深度学习基础知识/模型优化/image/L1.png" alt="image-20250315105347708" style="zoom:17%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Lasso</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">linear_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">linear_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">lasso_model</span> <span class="o">=</span> <span class="nc">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span> 
<span class="n">lasso_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">linear_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Linear Regression</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lasso_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Lasso Regression</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">L1 Regularization</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">./L1.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="2-l2-正则化ridge-正则化">2. <strong>L2 正则化（Ridge 正则化）</strong></h4>
<ul>
  <li>
    <p><strong>原理</strong>：L2 正则化通过在损失函数中加入参数的<strong>平方和</strong>来限制模型复杂度，形式为：
\(L(\theta) = \text{Loss}(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2\)</p>
  </li>
  <li><strong>作用</strong>：L2 正则化不会将权重完全推向零，而是<strong>缩小权重值</strong>。</li>
  <li><strong>应用</strong>：常用于回归问题中，尤其是在模型参数不多的情况下。</li>
</ul>

<blockquote>
  <p>L2正则化有助于<strong>避免模型对某些特征过度依赖</strong>，从而提升模型的稳定性和泛化能力。</p>

  <p><strong>几何直观：</strong> L2 正则化在参数空间中的惩罚是一个<strong>圆形</strong>（在二维空间下）。与 L1 正则化的菱形不同，圆形的最优解通常不会正好位于坐标轴上，这意味着它倾向于将参数值<strong>均匀缩小</strong>，而不是使某些参数为零。</p>
</blockquote>

<p><img src="/Users/rayss/Public/work/AI求职/study/深度学习基础知识/模型优化/image/L2.png" alt="image-20250315105347709" style="zoom: 20%;" /></p>

<hr />

<blockquote>
  <p>L1和L2的差异的根本原因在于<strong>绝对值函数</strong>和<strong>平方函数</strong>的数学性质不同。绝对值函数在零点的导数不连续，这会导致某些参数的权重变为零，而平方函数的导数则是连续的，它倾向于均匀地缩小所有参数值，而不是使某些参数完全为零</p>
</blockquote>

<hr />

<h4 id="3-弹性网正则化elastic-net">3. <strong>弹性网正则化（Elastic Net）</strong></h4>
<ul>
  <li>
    <p><strong>原理</strong>：弹性网正则化结合了 L1 和 L2 正则化的特点。它在损失函数中同时加入 L1 和 L2 正则化项，形式为：
\(L(\theta) = \text{Loss}(\theta) + \lambda_1 \sum_{i=1}^{n} |\theta_i| + \lambda_2 \sum_{i=1}^{n} \theta_i^2\)</p>
  </li>
  <li><strong>作用</strong>：弹性网正则化同时具备 <strong>L1 的稀疏性</strong>和 <strong>L2 的稳定性</strong>，因此它在处理大量特征时具有较好的表现，尤其是在特征间存在相关性的情况下。</li>
  <li><strong>应用</strong>：常用于特征选择和高维数据中。</li>
</ul>

<p><img src="/Users/rayss/Public/work/AI求职/study/深度学习基础知识/模型优化/image/E.png" alt="image-20250315105347710" style="zoom:17%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span><span class="n">Lasso</span> <span class="p">,</span><span class="n">Ridge</span><span class="p">,</span> <span class="n">ElasticNet</span>

<span class="c1"># 生成带有噪声的线性数据集
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 特征
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># 标签
</span>
<span class="c1"># 没有使用 L2 正则化的线性回归模型
</span><span class="n">linear_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">linear_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">lasso_model</span> <span class="o">=</span> <span class="nc">Lasso</span><span class="p">()</span>
<span class="n">lasso_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 使用 L2 正则化的 Ridge 回归模型
</span><span class="n">ridge_model</span> <span class="o">=</span> <span class="nc">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># 正则化参数 alpha
</span><span class="n">ridge_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">elastic_model</span> <span class="o">=</span> <span class="nc">ElasticNet</span><span class="p">()</span>
<span class="n">elastic_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># 可视化结果
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">linear_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Linear Regression (No Regularization)</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># plt.plot(X, lasso_model.predict(X), color='purple', linewidth=2, label='L1 Regression')
# plt.plot(X, ridge_model.predict(X), color='green', linewidth=2, label='Ridge Regression (L2 Regularization)')
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">elastic_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Elastic Regression (E Regularization)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">E Regularization</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">./E.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="4-早停early-stopping">4. <strong>早停（Early Stopping）</strong></h4>
<ul>
  <li><strong>原理</strong>：早停是一种通过<strong>监控训练误差和验证误差</strong>的变化来控制训练过程的方法。如果<strong>在训练过程中，验证误差开始增加，而训练误差继续减小，就会停止训练</strong>，防止过拟合。</li>
  <li><strong>作用</strong>：避免模型在训练数据上过度拟合，通过找到合适的训练时机，确保模型在未见过的数据上表现良好。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">class</span> <span class="nc">EarlyStopping</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="sh">'</span><span class="s">checkpoint.pt</span><span class="sh">'</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        EarlyStopping 类用于实现早停机制
        - patience: 容忍多少个 epoch 验证集损失没有改进
        - min: 验证集损失的最小变化量（只有超过此值才算有改进）
        - verbose: 是否打印早停信息
        - path: 保存最优模型的路径
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">min</span> <span class="o">=</span> <span class="nb">min</span>
        <span class="n">self</span><span class="p">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="n">self</span><span class="p">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">early_stop</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">best_model</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        调用此方法来检查是否应该提前停止训练
        - val_loss: 当前 epoch 验证集上的损失
        - model: 当前训练的模型
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">-</span> <span class="n">val_loss</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="nb">min</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># 保存最优模型
</span>            <span class="n">self</span><span class="p">.</span><span class="n">best_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">EarlyStopping counter: </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">counter</span><span class="si">}</span><span class="s"> out of </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">patience</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">counter</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">patience</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">early_stop</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Early stopping triggered</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">early_stop</span>

<span class="c1"># 训练过程中的早停实现
</span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># train code... 
</span>        <span class="c1"># 检查是否触发早停
</span>        <span class="k">if</span> <span class="nf">early_stopping</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Early stopping...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">early_stopping</span><span class="p">.</span><span class="n">best_model</span><span class="p">)</span>  <span class="c1"># 恢复最好的模型
</span>            <span class="k">break</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<h4 id="5-dropout">5. <strong>Dropout</strong></h4>
<ul>
  <li><strong>原理</strong>：Dropout 是一种常用的正则化方法，特别是在深度神经网络中。它在训练过程中<strong>随机“丢弃”神经网络中的一些神经元</strong>，即在每次训练时，随机选择一部分神经元的输出设置为零，从而减少神经网络对某些特定神经元的依赖。</li>
  <li><strong>作用</strong>：通过随机丢弃神经元，防止神经网络过度依赖某些特征，提高泛化能力。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="6-数据增强data-augmentation">6. <strong>数据增强（Data Augmentation）</strong></h4>
<ul>
  <li><strong>原理</strong>：数据增强是一种通过对训练数据进行变换（如旋转、平移、裁剪、缩放等）来生成新的训练样本，从而增加数据的多样性，防止模型对特定训练数据的过拟合。</li>
  <li><strong>作用</strong>：通过人为增加训练数据的多样性，使模型更好地泛化到未见过的数据。</li>
</ul>

<h4 id="7-权重衰减weight-decay">7. <strong>权重衰减（Weight Decay）</strong></h4>
<ul>
  <li><strong>原理</strong>：权重衰减是一种通过在损失函数中增加权重的平方和来约束模型的参数，使得权重值变得更小，从而减少过拟合的可能性。</li>
  <li><strong>作用</strong>：与 L2 正则化相似，权重衰减有助于防止模型过于复杂，避免过拟合。</li>
</ul>

<blockquote>
  <p>正则化方法中的超参数 λ 控制正则化项的权重。较大的 λ 会施加更强的正则化，从而使模型更加简化；较小的 λ 则允许模型更复杂。需要通过交叉验证等方法来选择一个合适的 λ 值，以平衡模型的拟合能力和泛化能力。</p>
</blockquote>


          </div>
          <footer class="post-footer">
            <div class="post-navigation">
              <a href="/articles/" class="nav-link">← 返回文章列表</a>
              <a href="/" class="nav-link">返回首页 →</a>
            </div>
          </footer>
        </article>
      </main>
      
      <footer class="site-footer">
        <div class="offsite-links">
          &copy; Scriptor all rights reserved. Theme by [JustGoodThemes](https://www.justgoodthemes.com).
        </div>
      </footer>
    </div>
  </div>
  
  <!-- Enhanced Prism.js with all plugins -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/normalize-whitespace/prism-normalize-whitespace.min.js"></script>
  
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>
  
  <style>
    /* Enhanced styles for markdown content */
    .markdown-content {
      line-height: 1.8;
    }
    
    .markdown-content pre {
      position: relative;
      margin: 1.5em 0;
      border-radius: 8px;
      overflow: visible;
    }
    
    .markdown-content pre[class*="language-"] {
      padding: 1em;
      margin: 1.5em 0;
      background: #2d3748;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .markdown-content code[class*="language-"] {
      font-size: 0.9em;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    }
    
    .markdown-content :not(pre) > code {
      background: #f1f5f9;
      color: #e53e3e;
      padding: 0.1em 0.3em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    
    .markdown-content blockquote {
      border-left: 4px solid #2d72d9;
      margin: 1.5em 0;
      padding: 0.5em 1em;
      background: #f8f9fa;
      border-radius: 0 4px 4px 0;
    }
    
    .markdown-content table {
      width: 100%;
      margin: 1.5em 0;
      border-collapse: collapse;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
      overflow: hidden;
    }
    
    .markdown-content th,
    .markdown-content td {
      padding: 12px 15px;
      text-align: left;
      border-bottom: 1px solid #e2e8f0;
    }
    
    .markdown-content th {
      background: #f8f9fa;
      font-weight: 600;
      color: #2d3748;
    }
    
    .markdown-content tr:hover {
      background: #f1f5f9;
    }
    
    /* Math formulas */
    .MathJax {
      font-size: 1.1em !important;
    }
    
    /* Mermaid diagrams */
    .mermaid {
      text-align: center;
      margin: 2em 0;
    }
    
    .post-tags {
      margin: 15px 0;
    }
    .tag {
      background: #e3f2fd;
      color: #1976d2;
      padding: 4px 8px;
      border-radius: 3px;
      font-size: 0.85em;
      margin-right: 8px;
    }
    .post-navigation {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
      padding-top: 20px;
      border-top: 1px solid #eee;
    }
    .nav-link {
      color: #2d72d9;
      text-decoration: none;
      padding: 8px 16px;
      border: 1px solid #2d72d9;
      border-radius: 4px;
      transition: all 0.3s ease;
    }
    .nav-link:hover {
      background: #2d72d9;
      color: white;
    }
    
    /* Code block enhancements */
    .line-numbers .line-numbers-rows {
      border-right: 1px solid #4a5568 !important;
    }
    
    .toolbar {
      position: absolute;
      top: 0.5em;
      right: 0.5em;
      z-index: 10;
    }
    
    .toolbar .toolbar-item button {
      background: #4a5568;
      border: none;
      color: white;
      padding: 0.25em 0.5em;
      border-radius: 3px;
      font-size: 0.8em;
      cursor: pointer;
    }
    
    .toolbar .toolbar-item button:hover {
      background: #2d3748;
    }
    
    @media (max-width: 600px) {
      .post-navigation {
        flex-direction: column;
        gap: 10px;
      }
      
      .markdown-content pre[class*="language-"] {
        margin: 1em -20px;
        border-radius: 0;
      }
      
      .markdown-content table {
        font-size: 0.9em;
      }
    }
  </style>
</body>
</html>
