<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Adam</title>
  <meta name="description" content="">
  <link rel="canonical" href="http://0.0.0.0:4000/articles/深度学习基础知识/模型优化/Adam/">
  <link href="/assets/css/style.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i%7CNoto+Serif:400,400i,700,700i&display=swap" rel="stylesheet">
  
  <!-- Enhanced Prism.js for comprehensive syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet">
  
  <!-- MathJax for math formulas -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <!-- Mermaid for diagrams -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      themeVariables: {
        primaryColor: '#2d72d9',
        primaryTextColor: '#333',
        primaryBorderColor: '#2d72d9',
        lineColor: '#666'
      }
    });
  </script>
</head>
<body>
  <div id="page" class="site">
    <div class="inner">
      <header class="site-header">
        <p class="site-title"><a class="logo-text" href="/">成都犀牛</a></p>
        <nav class="site-navigation">
          <div class="site-navigation-wrap">
            <ul class="menu">
              <li class="menu-item"><a href="/">Home</a></li>
              <li class="menu-item"><a href="/personal/">Personal</a></li>
              <li class="menu-item"><a href="/articles/">Articles</a></li>
            </ul>
          </div>
        </nav>
      </header>
      
      <main class="main-content fadeInDown delay_075s">
        <article class="post markdown-content">
          <header class="post-header">
            <h1 class="post-title">Adam</h1>
            
            
            
          </header>
          <div class="post-content">
            <blockquote>
  <p><strong>Adam</strong>（<code class="highlighter-rouge">Adaptive Moment Estimation</code>）是深度学习中广泛使用的一种优化算法，它结合了 <strong>Momentum</strong> 和 <strong>RMSprop</strong> 的优点，旨在提高梯度下降法的训练效率和稳定性。<strong>Adam 不仅考虑了梯度的平均值（动量），还考虑了梯度的平方的均值（自适应学习率）</strong>。它非常适用于大规模数据和参数的训练，能够自动调整学习率，减少人工调参的工作。</p>
</blockquote>

<h3 id="adam-的核心原理">Adam 的核心原理</h3>

<p>Adam 结合了以下几个关键概念：</p>

<ol>
  <li>
    <p><strong>一阶矩估计（Momentum）</strong>：估计梯度的均值（即动量），使得梯度更新过程中可以加速收敛。</p>
  </li>
  <li>
    <p><strong>二阶矩估计（RMSprop）</strong>：估计梯度的平方的均值，用于调整每个参数的学习率，使得大的梯度更新变得缓慢，小的梯度更新变得更快。</p>
  </li>
</ol>

<h3 id="adam-更新公式">Adam 更新公式</h3>

<p>Adam 的核心思想是同时计算梯度的<strong>一阶矩（均值）</strong>和<strong>二阶矩（方差）</strong>，并通过它们来更新参数。其具体更新过程如下：</p>

<ol>
  <li><strong>初始化</strong>：
    <ul>
      <li>初始化一阶矩和二阶矩的估计值为零。
        <ul>
          <li>( $m_0$ = 0 )</li>
          <li>( $v_0$ = 0 )</li>
        </ul>
      </li>
      <li>设置超参数：
        <ul>
          <li>学习率 ( $\eta$ )</li>
          <li>一阶矩衰减率 ( $\beta_1$ )</li>
          <li>二阶矩衰减率 ( $\beta_2$ )</li>
          <li>平滑项 ( $\epsilon$ )</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>计算梯度</strong>：在每次迭代中，计算当前参数的梯度 ( $\nabla \theta_t L(\theta_t) $)。</p>
  </li>
  <li>
    <p><strong>更新一阶矩估计</strong>（动量）：</p>

    <ul>
      <li>一阶矩的更新是对梯度的加权平均，公式为：</li>
    </ul>

\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla \theta_t L(\theta_t)\]

    <p>其中 ( $m_t$ ) 表示梯度的加权平均，( $\beta_1$ ) 是一阶矩的衰减率，通常取值在 0.9 附近。</p>
  </li>
  <li><strong>更新二阶矩估计</strong>（方差）：
    <ul>
      <li>二阶矩的更新是对梯度的平方进行加权平均，公式为：</li>
    </ul>

\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla \theta_t L(\theta_t))^2\]

    <p>其中 ( $v_t$ ) 表示梯度平方的加权平均，( $\beta_2$ ) 是二阶矩的衰减率，通常取值在 0.999 附近。</p>
  </li>
  <li>
    <p><strong>偏差校正</strong>：由于 ( $m_t$ ) 和 ( $v_t$ ) 是在初始阶段以 0 开始更新的，因此它们在训练初期会有偏差。为了纠正这个偏差，Adam 会对它们进行校正：</p>

    <ul>
      <li>偏差校正后的 ( $\hat{m}_t $) 和 ( $\hat{v}_t$ ) 为：</li>
    </ul>

\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}\]

\[\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\]

    <p>其中 ( $t$ ) 是当前的迭代步数。</p>
  </li>
  <li>
    <p><strong>更新参数</strong>：最后，根据校正后的 ( $\hat{m}_t$ ) 和 ( $\hat{v}_t$ ) 来更新参数：
\(\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\)</p>

    <p>其中 ( $\epsilon$ ) 是为了防止除零错误而添加的小常数（如 ( $10^{-8}$ )）。</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Adam
</span><span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">dx</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dx</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="adam-的优点">Adam 的优点</h3>

<ol>
  <li>
    <p><strong>自适应学习率</strong>：Adam 自动调整每个参数的学习率。对每个参数，Adam 根据其历史梯度的变化动态调整其更新步长，这使得它比传统的固定学习率的优化算法更有效，尤其是在处理稀疏梯度的情况下。</p>
  </li>
  <li>
    <p><strong>稳定性</strong>：结合了 <strong>动量</strong> 和 <strong>RMSprop</strong> 的优点，Adam 可以在不同的梯度尺度上稳定更新，从而加速收敛，并且能在训练过程中避免过大的震荡。</p>
  </li>
  <li>
    <p><strong>适应性强</strong>：对于大规模数据和高维参数，Adam 不需要很多手动调参。它的表现一般较好，尤其适合训练深度神经网络。</p>
  </li>
  <li>
    <p><strong>少量的超参数</strong>：与其他优化算法相比，Adam 的超参数相对较少，通常只需要调节学习率 ( $\eta$ )、一阶矩衰减率 ( $\beta_1$ )、二阶矩衰减率 ( $\beta_2$ ) 和平滑项 ($ \epsilon$ )。</p>
  </li>
</ol>

<h3 id="adam-的超参数">Adam 的超参数</h3>

<ul>
  <li>( $\eta$ )（学习率）：默认值通常为 0.001。</li>
  <li>( $\beta_1$ )（一阶矩衰减率）：默认值通常为 0.9。</li>
  <li>( $\beta_2$ )（二阶矩衰减率）：默认值通常为 0.999。</li>
  <li>( $\epsilon$ )（防止除零的小常数）：默认值通常为 ( $10^{-8}$ )。</li>
</ul>

<h3 id="adam-的缺点">Adam 的缺点</h3>

<ol>
  <li>
    <p><strong>对于某些问题不一定最优</strong>：虽然 Adam 在大多数问题上表现很好，但在一些特定任务（如训练非常深的网络或非常稀疏的任务）时，可能会表现出不稳定的行为。</p>
  </li>
  <li>
    <p><strong>可能会过拟合</strong>：由于 Adam 自动调整每个参数的学习率，这可能导致它在某些情况下过拟合训练数据，尤其是在小数据集上。</p>
  </li>
  <li>
    <p><strong>计算开销</strong>：由于需要计算梯度的平方和一阶矩与二阶矩的加权平均，Adam 相比于 SGD（随机梯度下降法）具有更高的计算开销。</p>
  </li>
</ol>

          </div>
          <footer class="post-footer">
            <div class="post-navigation">
              <a href="/articles/" class="nav-link">← 返回文章列表</a>
              <a href="/" class="nav-link">返回首页 →</a>
            </div>
          </footer>
        </article>
      </main>
      
      <footer class="site-footer">
        <div class="offsite-links">
          &copy; Scriptor all rights reserved. Theme by [JustGoodThemes](https://www.justgoodthemes.com).
        </div>
      </footer>
    </div>
  </div>
  
  <!-- Enhanced Prism.js with all plugins -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/normalize-whitespace/prism-normalize-whitespace.min.js"></script>
  
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>
  
  <style>
    /* Enhanced styles for markdown content */
    .markdown-content {
      line-height: 1.8;
    }
    
    .markdown-content pre {
      position: relative;
      margin: 1.5em 0;
      border-radius: 8px;
      overflow: visible;
    }
    
    .markdown-content pre[class*="language-"] {
      padding: 1em;
      margin: 1.5em 0;
      background: #2d3748;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .markdown-content code[class*="language-"] {
      font-size: 0.9em;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    }
    
    .markdown-content :not(pre) > code {
      background: #f1f5f9;
      color: #e53e3e;
      padding: 0.1em 0.3em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    
    .markdown-content blockquote {
      border-left: 4px solid #2d72d9;
      margin: 1.5em 0;
      padding: 0.5em 1em;
      background: #f8f9fa;
      border-radius: 0 4px 4px 0;
    }
    
    .markdown-content table {
      width: 100%;
      margin: 1.5em 0;
      border-collapse: collapse;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
      overflow: hidden;
    }
    
    .markdown-content th,
    .markdown-content td {
      padding: 12px 15px;
      text-align: left;
      border-bottom: 1px solid #e2e8f0;
    }
    
    .markdown-content th {
      background: #f8f9fa;
      font-weight: 600;
      color: #2d3748;
    }
    
    .markdown-content tr:hover {
      background: #f1f5f9;
    }
    
    /* Math formulas */
    .MathJax {
      font-size: 1.1em !important;
    }
    
    /* Mermaid diagrams */
    .mermaid {
      text-align: center;
      margin: 2em 0;
    }
    
    .post-tags {
      margin: 15px 0;
    }
    .tag {
      background: #e3f2fd;
      color: #1976d2;
      padding: 4px 8px;
      border-radius: 3px;
      font-size: 0.85em;
      margin-right: 8px;
    }
    .post-navigation {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
      padding-top: 20px;
      border-top: 1px solid #eee;
    }
    .nav-link {
      color: #2d72d9;
      text-decoration: none;
      padding: 8px 16px;
      border: 1px solid #2d72d9;
      border-radius: 4px;
      transition: all 0.3s ease;
    }
    .nav-link:hover {
      background: #2d72d9;
      color: white;
    }
    
    /* Code block enhancements */
    .line-numbers .line-numbers-rows {
      border-right: 1px solid #4a5568 !important;
    }
    
    .toolbar {
      position: absolute;
      top: 0.5em;
      right: 0.5em;
      z-index: 10;
    }
    
    .toolbar .toolbar-item button {
      background: #4a5568;
      border: none;
      color: white;
      padding: 0.25em 0.5em;
      border-radius: 3px;
      font-size: 0.8em;
      cursor: pointer;
    }
    
    .toolbar .toolbar-item button:hover {
      background: #2d3748;
    }
    
    @media (max-width: 600px) {
      .post-navigation {
        flex-direction: column;
        gap: 10px;
      }
      
      .markdown-content pre[class*="language-"] {
        margin: 1em -20px;
        border-radius: 0;
      }
      
      .markdown-content table {
        font-size: 0.9em;
      }
    }
  </style>
</body>
</html>
