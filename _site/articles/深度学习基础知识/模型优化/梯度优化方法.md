> **梯度优化算法发展历史**
>
> `SGD -> SGDM ->NAG -> AdaGrad -> AdaDelta -> Adam -> Nadam`
>
> - `SGD`：利用参数梯度和学习率调整梯度
> - `Momentum`：加入了历史梯度信息，将使用之前梯度的加权平均来更新当前的参数
> - `RMSprop`：通过梯度的平方的均值（梯度的波动情况）来调整每个参数的学习率
> - `Adam`: 将`Momentum`和`RMSprop`优点结合







