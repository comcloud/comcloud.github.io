

### 4.1 **Sigmoid函数**
公式： 
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$


**优点**：

- 输出范围在 \( (0, 1) \) 之间，适合二分类任务的概率输出。
- 函数平滑，具有良好的数学性质。

**缺点**：
- **梯度消失**：当输入值过大或过小时，梯度非常小，导致反向传播时更新缓慢。
- **输出不以零为中心**：会导致梯度下降时参数更新不平衡。

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 400)
y = sigmoid(x)

plt.plot(x, y)
plt.title("Sigmoid Function")
plt.xlabel("x")
plt.ylabel("sigmoid(x)")
plt.grid(True)
plt.show()
```

### 4.2 **Tanh函数**
公式： 
$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$
**优点**：
- 输出范围在 \( (-1, 1) \) 之间，输出对称于零。
- 比 Sigmoid 更好地避免梯度消失问题。

**缺点**：
- 对于大输入值，依然存在梯度消失问题。
- 计算复杂度相较于 Sigmoid 稍微高一点。

```python
def tanh(x):
    return np.tanh(x)

y = tanh(x)

plt.plot(x, y)
plt.title("Tanh Function")
plt.xlabel("x")
plt.ylabel("tanh(x)")
plt.grid(True)
plt.show()
```

### 4.3 **ReLU函数 (Rectified Linear Unit)**
公式： 
$$
\text{ReLU}(x) = \max(0, x)
$$
**优点**：
- 计算简单且高效，广泛应用于深度学习。
- 可以减少梯度消失问题。

**缺点**：
- **神经元死亡问题**：对于负数输入，ReLU 的输出始终为零，可能导致神经元在训练中“死亡”。
  
```python
def relu(x):
    return np.maximum(0, x)

y = relu(x)

plt.plot(x, y)
plt.title("ReLU Function")
plt.xlabel("x")
plt.ylabel("ReLU(x)")
plt.grid(True)
plt.show()
```

### 4.4 **Leaky ReLU函数**
公式： 
$$
\text{Leaky ReLU}(x) = \max(\alpha x, x)
$$
**优点**：
- 解决了 ReLU 的神经元死亡问题。即使输入小于零，输出也不会完全为零。
  

**缺点**：
- 参数 \(\alpha\) 的选择需要实验调优，且较为简单。

```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

y = leaky_relu(x)

plt.plot(x, y)
plt.title("Leaky ReLU Function")
plt.xlabel("x")
plt.ylabel("Leaky ReLU(x)")
plt.grid(True)
plt.show()
```

### 4.5 **PReLU函数 (Parametric ReLU)**
公式： 
$$
\text{PReLU}(x) = \max(\alpha x, x) \quad \text{with learnable} \ \alpha
$$


**优点**：

- \(\alpha\) 是可学习的，可以根据训练数据进行优化，解决了 Leaky ReLU 中固定 \(\alpha\) 的问题。

**缺点**：
- 增加了模型的复杂度，需要额外的训练。

```python
from keras.layers import PReLU

x = np.linspace(-10, 10, 400)
y = np.maximum(0.01 * x, x)  # Simulated PReLU behavior for illustration

plt.plot(x, y)
plt.title("PReLU Function")
plt.xlabel("x")
plt.ylabel("PReLU(x)")
plt.grid(True)
plt.show()
```

### 4.6 **ELU函数 (Exponential Linear Unit)**
公式： 
$$
\text{ELU}(x) = \begin{cases} 
x, & x > 0 \\
\alpha (e^x - 1), & x \leq 0 
\end{cases}
$$
**优点**：
- 输出负数的区域避免了死神经元问题。
- 提供了更加平滑的输出，且当 \(x < 0\) 时有非线性激活。

**缺点**：

- 计算较为复杂，可能导致训练时的计算开销较大。

```python
def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

y = elu(x)

plt.plot(x, y)
plt.title("ELU Function")
plt.xlabel("x")
plt.ylabel("ELU(x)")
plt.grid(True)
plt.show()
```

### 4.7 **SELU函数 (Scaled Exponential Linear Unit)**
公式： 
$$
\text{SELU}(x) = \lambda \times \text{ELU}(x)
$$
**优点**：
- 对于深层神经网络，能够自动进行自我归一化，避免梯度消失或爆炸问题。

**缺点**：
- 只适用于某些特定的网络架构（如自归一化网络）。
- 需要特定的初始化。

```python
def selu(x):
    return 1.0507 * elu(x, alpha=1.6733)

y = selu(x)

plt.plot(x, y)
plt.title("SELU Function")
plt.xlabel("x")
plt.ylabel("SELU(x)")
plt.grid(True)
plt.show()
```

### 4.8 **Swish函数**
公式： 
$$
\text{Swish}(x) = x \times \sigma(x)
$$
**优点**：
- 平滑且非单调，能够有效缓解梯度消失问题。
- 在很多任务中比 ReLU 和 Sigmoid 等表现更好。

**缺点**：
- 计算复杂度比 ReLU 要高。

```python
def swish(x):
    return x * sigmoid(x)

y = swish(x)

plt.plot(x, y)
plt.title("Swish Function")
plt.xlabel("x")
plt.ylabel("Swish(x)")
plt.grid(True)
plt.show()
```

### 4.9 **Mish函数**
公式： 
$$
\text{Mish}(x) = x \times \tanh(\text{softplus}(x))
$$

$$
softplus = ( \ln(1 + e^x)
$$

**优点**：
- 提供更好的平滑性和非线性，能够在多个深度学习任务中取得更好的性能。
  

**缺点**：
- 计算复杂度相较于 ReLU 更高。

```python
def mish(x):
    return x * np.tanh(np.log1p(np.exp(x)))

y = mish(x)

plt.plot(x, y)
plt.title("Mish Function")
plt.xlabel("x")
plt.ylabel("Mish(x)")
plt.grid(True)
plt.show()
```

### 4.10 **Softmax函数**
公式： 
$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$
**优点**：
- 常用于多分类问题，能够将输出转换为概率分布。

**缺点**：
- 对于类别数目较多时，计算可能较为复杂，且容易受到极端输入值的影响。

```python
def softmax(x):
    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability
    return e_x / np.sum(e_x)

x = np.linspace(-2, 2, 10)
y = softmax(x)

plt.plot(x, y)
plt.title("Softmax Function")
plt.xlabel("x")
plt.ylabel("Softmax(x)")
plt.grid(True)
plt.show()
```

### 4.11 **SwiGLU函数**
公式： 
$$
\text{SwiGLU}(x) = \text{ReLU}(x) \times \text{Sigmoid}(x)
$$
**优点**：
- 结合了 ReLU 和 Sigmoid 的优点，能够在某些任务中提供更好的非线性表现。

**缺点**：

- 计算复杂度稍高于单独的 ReLU 或 Sigmoid。

```python
def swiglu(x):
    return relu(x) * sigmoid(x)

y = swiglu(x)

plt.plot(x, y)
plt.title("SwiGLU Function")
plt.xlabel("x")
plt.ylabel("SwiGLU(x)")
plt.grid(True)
plt.show()
```

