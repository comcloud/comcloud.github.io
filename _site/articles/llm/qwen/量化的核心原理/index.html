<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>量化的核心原理</title>
  <meta name="description" content="">
  <link rel="canonical" href="http://localhost:4000/articles/llm/qwen/量化的核心原理/">
  <link href="/assets/css/style.css" rel="stylesheet">
  <link href="/assets/css/temp-style.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i%7CNoto+Serif:400,400i,700,700i&display=swap" rel="stylesheet">
  
  <!-- Enhanced Prism.js for comprehensive syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet">
  
  <!-- MathJax for math formulas -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <!-- Mermaid for diagrams -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      themeVariables: {
        primaryColor: '#2d72d9',
        primaryTextColor: '#333',
        primaryBorderColor: '#2d72d9',
        lineColor: '#666'
      }
    });
  </script>
</head>
<body>
  <div id="page" class="site">
    <div class="inner">
      <header class="site-header">
        <p class="site-title"><a class="logo-text" href="/">成都犀牛</a></p>
        <nav class="site-navigation">
          <div class="site-navigation-wrap">
            <ul class="menu">
              <li class="menu-item"><a href="/">Home</a></li>
              <li class="menu-item"><a href="/personal/">Personal</a></li>
              <li class="menu-item"><a href="/articles/">Articles</a></li>
            </ul>
          </div>
        </nav>
      </header>
      
      <main class="main-content fadeInDown delay_075s">
        <article class="post article-content">
          <header class="post-header">
            <h1 class="post-title">量化的核心原理</h1>
            
            
            
          </header>
          <div class="post-content">
            <hr />
<p>title: “量化的核心原理”
description: “深入讲解神经网络量化的数学原理、权重量化运算机制”
date: 2024-08-03
tags: [“Qwen”, “量化”, “数学原理”, “深度学习”]
category: “LLM”
layout: markdown
—</p>

<h3 id="量化的核心原理浮点数到整数的映射">量化的核心原理：浮点数到整数的映射</h3>

<p>量化的基本思想是将神经网络中通常使用的浮点数（如FP32，32位浮点数）表示的权重和激活值，映射到低位宽的整数（如INT8，8位整数）表示。这个映射过程需要定义一个<strong>量化函数</strong>和一个<strong>反量化函数</strong>。</p>

<p>最常见的量化方法是<strong>线性均匀量化 (Linear Uniform Quantization)</strong>，它通过一个<strong>缩放因子 (Scale Factor, \S)</strong> 和一个<strong>零点 (Zero Point, \Z)</strong> 来实现浮点数和整数之间的线性映射。</p>

<h4 id="1-量化参数-s-和-z-的确定">1. 量化参数 (<strong>S</strong> 和 <strong>Z</strong>) 的确定</h4>

<p>在进行量化之前，我们需要确定浮点数范围（$F_{min}$,$F_{max}$）和整数范围（$Q_{min}$,$Q_{max}$）。</p>

<ul>
  <li><strong>整数范围</strong>通常是固定的。例如，对于 8 位无符号整数<code class="highlighter-rouge"> (UINT8)</code>$Q_{min}=0$,$Q_{max}=255$；对于 8 位有符号整数 <code class="highlighter-rouge">(INT8)</code>，$Q_{min}=-128$,$Q_{max}=127$。</li>
  <li><strong>浮点数范围</strong>则需要从模型中获取。对于权重，可以通过其最小值和最大值直接确定。对于激活值，由于它们是动态变化的，通常需要在推理前通过<strong>校准 (Calibration)</strong> 过程，用一小部分代表性数据运行模型，收集激活值的统计信息（如最小值、最大值或均值、方差）来确定。</li>
</ul>

<p>一旦确定了浮点数的范围 ($F_{min}$,$F_{max}$） 和整数的范围 ($Q_{min}$,$Q_{max}$），就可以计算 S 和 Z：</p>

<ul>
  <li>
    <p>缩放因子 S (Scale Factor):
\(S=(Q_{max}−Q_{min})/ (F_{max}−F_{min})\)</p>

    <p>这个 S 表示浮点数单位的变化对应整数单位的变化量。</p>
  </li>
  <li>
    <p>零点 Z (Zero Point):</p>

    <p>Z=Qmin−SFmin</p>

    <p>零点 Z 是为了将浮点数 0.0 映射到某个整数值。它是一个整数，通过四舍五入得到。</p>
  </li>
</ul>

<h4 id="2-量化过程-quantization">2. 量化过程 (Quantization)</h4>

<p>将一个浮点数 FP 量化为整数 Q 的公式：</p>

<p>Q=round(SFP+Z)</p>

<p>其中 round 表示四舍五入。</p>

<h4 id="3-反量化过程-dequantization">3. 反量化过程 (Dequantization)</h4>

<p>在某些情况下，为了进行计算或输出，需要将整数 Q 反量化回浮点数 FPreconstructed。</p>

<p>FPreconstructed=(Q−Z)×S</p>

<p><strong>为什么需要零点 *<em>Z*</em>？</strong></p>

<p>零点的存在是为了确保浮点数 0.0 能够精确地映射到一个整数值（通常是 Qmin 或 Qmax 或中间某个值），这对于激活函数（如 ReLU，其中大量值为 0）和稀疏权重矩阵非常重要，可以避免量化误差累积。</p>

<h3 id="权重量化运算如何进行">权重量化运算如何进行？</h3>

<p>模型参数（权重）的量化比激活值相对简单，因为权重是固定的。主要有两种策略：</p>

<h4 id="1-离线量化-offline-quantization--ptq-post-training-quantization">1. 离线量化 (Offline Quantization) / PTQ (Post-Training Quantization)</h4>

<p>这是最常见的权重量化方式，特别是在部署阶段。</p>

<ol>
  <li>
    <p><strong>加载预训练的 FP32 模型。</strong></p>
  </li>
  <li>
    <p>逐层或逐通道地计算每个权重张量的 <strong>Fmin</strong> 和 <strong>Fmax</strong>。</p>

    <ul>
      <li>
        <p>例如，对于一个卷积核</p>

        <p>W∈RCout×Cin×KH×KW</p>

        <p>：</p>

        <ul>
          <li><strong>逐层量化：</strong> 找到整个 W 的最小值 Wmin 和最大值 Wmax。</li>
          <li><strong>逐通道量化：</strong> 对于每个输出通道 Cout，单独计算其对应权重的最小值和最大值，得到 Cout 组 (Sc,Zc)。这种方式精度通常更高。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>根据 *<em>Fmin,Fmax*</em> 和目标整数范围，计算出对应的 *<em>S*</em> 和 *<em>Z*</em>。</strong></p>
  </li>
  <li>
    <p><strong>使用 *<em>S*</em> 和 *<em>Z*</em> 将每个浮点权重 *<em>Wfp*</em> 量化为整数 *<em>Wq*</em>。</strong> Wq=round(SWfp+Z)</p>
  </li>
  <li>
    <p><strong>存储量化后的模型：</strong> 此时模型的权重都是 INT8 (或 INT4) 整数。</p>
  </li>
</ol>

<p><strong>推理时权重的运算：</strong></p>

<p>在推理时，量化后的权重 Wq 会直接参与计算。但在实际的硬件或推理引擎中，运算通常仍然是浮点运算（或者更精确地说是模拟浮点运算）。</p>

<p>考虑一个简单的乘法：Y=W×X</p>

<p>在量化后，变成：Yfp≈(Wq−Zw)×Sw×(Xq−Zx)×Sx</p>

<p>这个浮点乘法可以分解为整数运算：</p>

<p>Yfp≈(Wq⋅Xq−Wq⋅Zx−Xq⋅Zw+Zw⋅Zx)⋅Sw⋅Sx</p>

<p>为了避免频繁的反量化和浮点乘法，高性能的推理引擎（如 TensorRT）会进行<strong>算子融合 (Operator Fusion)</strong> 和<strong>整数化计算 (Integer-only Arithmetic)</strong>。它们会把 Sw⋅Sx 作为一个<strong>重标度因子 (rescale factor)</strong> 传递，并在中间结果中累积。最终的输出会在一个更高的整数精度（如 INT32）中进行累加，然后通过最终的缩放和零点转换为下一个 INT8 激活值或最终的 FP32 输出。</p>

<p><strong>核心思想是：</strong> 尽量在整数域内进行计算，只在必要时（如层与层之间传递激活值或最终输出）进行反量化和重新量化。</p>

<h4 id="2-在线量化-online-quantization--qat-quantization-aware-training">2. 在线量化 (Online Quantization) / QAT (Quantization-Aware Training)</h4>

<p>QAT 则是在训练过程中引入量化操作的模拟。</p>

<ol>
  <li>在训练图 (Computation Graph) 中插入伪量化 (Fake Quantization) 节点。
    <ul>
      <li>这些伪量化节点在正向传播时，会执行 <strong>量化 -&gt; 反量化</strong> 的操作。也就是说，浮点权重先被量化成低精度整数，然后立即反量化回浮点数。</li>
      <li>这样，模型在训练时，其浮点权重虽然仍然是 FP32，但它们会“感知”到低精度量化带来的误差，并学习如何去弥补这些误差。</li>
    </ul>
  </li>
  <li><strong>反向传播时，梯度仍然是针对 FP32 权重计算的。</strong> 伪量化操作通常是可导的（或者通过梯度旁路，Straight-Through Estimator, STE）。</li>
  <li><strong>训练结束后，直接保存模型中的 FP32 权重。</strong> 这些权重已经适应了量化误差。</li>
  <li><strong>在部署时，直接对这些训练好的 FP32 权重进行最终的量化</strong>（类似 PTQ 的步骤 2-4），将它们转换为 INT8。由于权重已经适应了量化，所以精度损失很小。</li>
</ol>

<p><strong>总结权重量化运算的原理：</strong></p>

<p>无论是 PTQ 还是 QAT，核心都是找到一个合适的线性映射关系（由 S 和 Z 定义），将原始浮点数范围映射到目标整数范围。在推理时，通过硬件支持的整数乘加指令，结合预先计算好的 S 和 Z，可以在整数域内高效地完成大部分计算，从而达到加速和减少资源占用的目的。对于大模型来说，由于其庞大的参数量，即使是 8-bit 或 4-bit 量化，也能带来巨大的内存和计算效益。</p>

          </div>
          <footer class="post-footer">
            <div class="post-navigation">
              <a href="/articles/" class="nav-link">← 返回文章列表</a>
              <a href="/" class="nav-link">返回首页 →</a>
            </div>
          </footer>
        </article>
      </main>
      
      <footer class="site-footer">
        <div class="offsite-links">
          &copy; Scriptor all rights reserved. Theme by [JustGoodThemes](https://www.justgoodthemes.com).
        </div>
      </footer>
    </div>
  </div>
  
  <!-- Enhanced Prism.js with all plugins -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/normalize-whitespace/prism-normalize-whitespace.min.js"></script>
  
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>
  
  <style>
    /* Enhanced styles for markdown content */
    .markdown-content {
      line-height: 1.8;
    }
    
    .markdown-content pre {
      position: relative;
      margin: 1.5em 0;
      border-radius: 8px;
      overflow: visible;
    }
    
    .markdown-content pre[class*="language-"] {
      padding: 1em;
      margin: 1.5em 0;
      background: #2d3748;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .markdown-content code[class*="language-"] {
      font-size: 0.9em;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    }
    
    .markdown-content :not(pre) > code {
      background: #f1f5f9;
      color: #e53e3e;
      padding: 0.1em 0.3em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    
    .markdown-content blockquote {
      border-left: 4px solid #2d72d9;
      margin: 1.5em 0;
      padding: 0.5em 1em;
      background: #f8f9fa;
      border-radius: 0 4px 4px 0;
    }
    
    .markdown-content table {
      width: 100%;
      margin: 1.5em 0;
      border-collapse: collapse;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
      overflow: hidden;
    }
    
    .markdown-content th,
    .markdown-content td {
      padding: 12px 15px;
      text-align: left;
      border-bottom: 1px solid #e2e8f0;
    }
    
    .markdown-content th {
      background: #f8f9fa;
      font-weight: 600;
      color: #2d3748;
    }
    
    .markdown-content tr:hover {
      background: #f1f5f9;
    }
    
    /* Math formulas */
    .MathJax {
      font-size: 1.1em !important;
    }
    
    /* Mermaid diagrams */
    .mermaid {
      text-align: center;
      margin: 2em 0;
    }
    
    .post-tags {
      margin: 15px 0;
    }
    .tag {
      background: #e3f2fd;
      color: #1976d2;
      padding: 4px 8px;
      border-radius: 3px;
      font-size: 0.85em;
      margin-right: 8px;
    }
    .post-navigation {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
      padding-top: 20px;
      border-top: 1px solid #eee;
    }
    .nav-link {
      color: #2d72d9;
      text-decoration: none;
      padding: 8px 16px;
      border: 1px solid #2d72d9;
      border-radius: 4px;
      transition: all 0.3s ease;
    }
    .nav-link:hover {
      background: #2d72d9;
      color: white;
    }
    
    /* Code block enhancements */
    .line-numbers .line-numbers-rows {
      border-right: 1px solid #4a5568 !important;
    }
    
    .toolbar {
      position: absolute;
      top: 0.5em;
      right: 0.5em;
      z-index: 10;
    }
    
    .toolbar .toolbar-item button {
      background: #4a5568;
      border: none;
      color: white;
      padding: 0.25em 0.5em;
      border-radius: 3px;
      font-size: 0.8em;
      cursor: pointer;
    }
    
    .toolbar .toolbar-item button:hover {
      background: #2d3748;
    }
    
    @media (max-width: 600px) {
      .post-navigation {
        flex-direction: column;
        gap: 10px;
      }
      
      .markdown-content pre[class*="language-"] {
        margin: 1em -20px;
        border-radius: 0;
      }
      
      .markdown-content table {
        font-size: 0.9em;
      }
    }
  </style>
</body>
</html>
