<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>量化的核心原理</title>
  <meta name="description" content="深入讲解神经网络量化的数学原理、权重量化运算机制">
  <link rel="canonical" href="http://0.0.0.0:4000/quantization-principles/">
  <link rel="alternate" type="application/rss+xml" title="成都犀牛 Feed"
    href="http://0.0.0.0:4000/feed.xml">
  
  <link rel="shortcut icon" href="/images/favicon.png" type="image/png" />
  
  <!-- Modern Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
  
  <!-- Styles -->
  <link href="/assets/css/style.css" rel="stylesheet">
  
  <!-- Meta tags for better SEO and social sharing -->
  <meta property="og:title" content="量化的核心原理">
  <meta property="og:description" content="深入讲解神经网络量化的数学原理、权重量化运算机制">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://0.0.0.0:4000/quantization-principles/">
  <meta name="twitter:card" content="summary_large_image">
</head>
<body>

  <div id="page" class="site">
    <header class="site-header">
  
  <p class="site-title"><a class="logo-text" href="/">成都犀牛</a></p>
  
  <nav class="site-navigation">
    <div class="site-navigation-wrap">
      <h2 class="screen-reader-text">Main navigation</h2>
      <ul class="menu">
        
        
        
        <li class="menu-item ">
          <a class="" href="/">Home</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/personal/">Personal</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/articles/">Articles</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="https://github.com/comcloud">comcloud</a>
        </li>
        
      </ul><!-- .menu -->
      <button id="menu-close" class="menu-toggle"><span class="screen-reader-text">Close Menu</span><span
          class="icon-close" aria-hidden="true"></span></button>
    </div><!-- .site-navigation-wrap -->
  </nav><!-- .site-navigation -->
  <button id="menu-open" class="menu-toggle"><span class="screen-reader-text">Open Menu</span><span class="icon-menu" aria-hidden="true"></span></button>
</header>


    <main class="site-main">
      <main class="main-content fadeInDown delay_075s">
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">量化的核心原理</h1>
      
      <p class="post-description">深入讲解神经网络量化的数学原理、权重量化运算机制</p>
      
      
      <div class="post-meta">
        <time class="post-date">August 3, 2024</time>
      </div>
      
      
    </header><!-- .post-header -->
    
    <div class="post-content">
      <h1 id="量化的核心原理浮点数到整数的映射">量化的核心原理：浮点数到整数的映射</h1>

<p>量化的基本思想是将神经网络中通常使用的浮点数（如FP32，32位浮点数）表示的权重和激活值，映射到低位宽的整数（如INT8，8位整数）表示。这个映射过程需要定义一个<strong>量化函数</strong>和一个<strong>反量化函数</strong>。</p>

<p>最常见的量化方法是<strong>线性均匀量化 (Linear Uniform Quantization)</strong>，它通过一个<strong>缩放因子 (Scale Factor, S)</strong> 和一个<strong>零点 (Zero Point, Z)</strong> 来实现浮点数和整数之间的线性映射。</p>

<h2 id="1-量化参数-s-和-z-的确定">1. 量化参数 (S 和 Z) 的确定</h2>

<p>在进行量化之前，我们需要确定浮点数范围（$F_{min}$,$F_{max}$）和整数范围（$Q_{min}$,$Q_{max}$）。</p>

<ul>
  <li><strong>整数范围</strong>通常是固定的。例如，对于 8 位无符号整数 <code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>(UINT8)
</pre></td></tr></tbody></table></code> $Q_{min}=0$,$Q_{max}=255$；对于 8 位有符号整数 <code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>(INT8)
</pre></td></tr></tbody></table></code>，$Q_{min}=-128$,$Q_{max}=127$。</li>
  <li><strong>浮点数范围</strong>则需要从模型中获取。</li>
</ul>

<p>一旦确定了浮点数的范围，就可以计算 S 和 Z：</p>

<ul>
  <li>
    <p>缩放因子 S (Scale Factor):
\(S=(Q_{max}−Q_{min})/ (F_{max}−F_{min})\)</p>
  </li>
  <li>
    <p>零点 Z (Zero Point):
\(Z=Q_{min}−S \\times F_{min}\)</p>
  </li>
</ul>

<h2 id="2-量化过程-quantization">2. 量化过程 (Quantization)</h2>

<p>将一个浮点数 FP 量化为整数 Q 的公式：</p>

<p>[Q=round(S \times FP+Z)]</p>

<p>其中 round 表示四舍五入。</p>

<h2 id="3-反量化过程-dequantization">3. 反量化过程 (Dequantization)</h2>

<p>在某些情况下，为了进行计算或输出，需要将整数 Q 反量化回浮点数：</p>

<p>[FP_{reconstructed}=(Q−Z) \times S]</p>

<p><strong>为什么需要零点 Z？</strong></p>

<p>零点的存在是为了确保浮点数 0.0 能够精确地映射到一个整数值，这对于激活函数（如 ReLU，其中大量值为 0）和稀疏权重矩阵非常重要，可以避免量化误差累积。</p>

<h2 id="权重量化运算">权重量化运算</h2>

<p>模型参数（权重）的量化比激活值相对简单，因为权重是固定的。主要有两种策略：</p>

<h3 id="离线量化-ptq---post-training-quantization">离线量化 (PTQ - Post-Training Quantization)</h3>

<p>这是最常见的权重量化方式：</p>

<ol>
  <li>加载预训练的 FP32 模型</li>
  <li>计算每个权重张量的最小值和最大值</li>
  <li>根据范围计算对应的 S 和 Z</li>
  <li>将每个浮点权重量化为整数</li>
  <li>存储量化后的模型</li>
</ol>

<h3 id="在线量化-qat---quantization-aware-training">在线量化 (QAT - Quantization-Aware Training)</h3>

<p>在训练过程中引入量化操作的模拟：</p>

<ol>
  <li>在训练图中插入伪量化节点</li>
  <li>执行量化→反量化操作来模拟量化误差</li>
  <li>模型学习适应量化带来的误差</li>
  <li>部署时直接进行最终量化</li>
</ol>

<hr />

<p>📚 <strong>相关文章推荐：</strong></p>
<ul>
  <li><a href="/llama-series/">Llama系列大模型解析</a></li>
  <li><a href="/deepseek-intro/">DeepSeek模型介绍</a></li>
  <li><a href="/blog/">回到博客首页</a></li>
</ul>

    </div><!-- .post-content -->
    
    
    <footer class="post-footer">
      <div class="post-tags">
        <span class="tags-label">Tags:</span>
        
        <a href="/tags#qwen" class="tag-link">Qwen</a>, 
        
        <a href="/tags#%E9%87%8F%E5%8C%96" class="tag-link">量化</a>, 
        
        <a href="/tags#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86" class="tag-link">数学原理</a>, 
        
        <a href="/tags#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="tag-link">深度学习</a>
        
      </div>
    </footer>
    
  </article><!-- .post -->
</main><!-- .site-main -->
    </main>
    <footer class="site-footer">
  <div class="offsite-links">
    
      
<a href="https://twitter.com/" target="_blank" rel="noopener">
  <span class="fa-twitter" aria-hidden="true"></span>
  <span class="screen-reader-text">Twitter</span>
</a>

<a href="https://github.com/" target="_blank" rel="noopener">
  <span class="fa-github" aria-hidden="true"></span>
  <span class="screen-reader-text">GitHub</span>
</a>

<a href="https://www.instagram.com/" target="_blank" rel="noopener">
  <span class="fa-instagram" aria-hidden="true"></span>
  <span class="screen-reader-text">Instagram</span>
</a>

<a href="https://www.linkedin.com/" target="_blank" rel="noopener">
  <span class="fa-linkedin" aria-hidden="true"></span>
  <span class="screen-reader-text">LinkedIn</span>
</a>

    
  </div><!-- .offsite-links -->
  <div class="footer-bottom">
    <div class="site-info">
      <p>© Scriptor all rights reserved. Theme by <a href="https://www.justgoodthemes.com">JustGoodThemes</a>.</p>

    </div><!-- .site-info -->
    <a href="#page" id="back-to-top" class="back-to-top"><span class="screen-reader-text">Back to the top </span>&#8593;</a>
  </div><!-- .footer-bottom -->
</footer><!-- .site-footer -->

  </div><!-- .site -->

  <!-- Scripts -->
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>
  
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>

</body>
</html>