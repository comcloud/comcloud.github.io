I"<blockquote>
  <p>动量法（Momentum）是一种优化算法，通常用于加速梯度下降法的收敛速度，减少震荡，特别是在训练深度学习模型时常常使用。它通过<strong>将前一次更新的方向“记住”一段时间</strong>，帮助模型沿着合适的方向更快地收敛。</p>
</blockquote>

<h3 id="动量法的基本思想">动量法的基本思想</h3>

<p>在标准的梯度下降法中，模型的参数更新是完全依赖于当前梯度的，假设在某个方向上反复调整时，梯度下降会经历较大的波动和震荡。而动量法通过给每次参数更新赋予一定的“惯性”，即<strong>使用之前梯度的加权平均来更新当前的参数</strong>。这样，参数更新时不仅考虑当前的梯度，还包含了历史梯度的影响，从而平滑了更新过程，避免了不必要的波动。</p>

<h3 id="动量法的公式">动量法的公式</h3>

<p>假设我们要最小化一个损失函数 ( $L(\theta)$ )，其中 ( $\theta$ ) 是模型的参数，( $\nabla L(\theta) $) 是损失函数相对于参数的梯度。使用动量法的参数更新公式如下：</p>

<ol>
  <li>
    <p><strong>更新速度</strong>（velocity）:
\(v_{t+1} = \beta v_t + (1 - \beta) \nabla L(\theta_t)\)</p>

    <p>其中，( $v_t$ ) 是上一次的速度（即梯度的加权平均），( $\beta$ ) 是动量因子，通常设置为一个接近 1 的值（例如 0.9），表示历史梯度对当前梯度的影响。</p>
  </li>
  <li>
    <p><strong>更新参数</strong>:
\(\theta_{t+1} = \theta_t - \alpha v_{t+1}\)</p>

    <p>其中，( $\alpha$ ) 是学习率，( $\theta_t$ ) 是当前的参数值，( $v_{t+1}$ ) 是经过动量调整后的更新方向。</p>
  </li>
</ol>

<h3 id="动量法的作用">动量法的作用</h3>

<ol>
  <li><strong>加速收敛</strong>：
    <ul>
      <li>在参数更新时，动量法考虑了历史梯度的累积，从而使得模型在沿着梯度下降的方向上更加快速地前进，尤其是在梯度变化比较平稳的区域。</li>
    </ul>
  </li>
  <li><strong>减少震荡</strong>：
    <ul>
      <li>如果梯度下降的方向存在一些小的周期性震荡（例如梯度在某些维度上波动较大），动量法可以通过考虑历史梯度的方向和大小来减少这种震荡，使得参数更新更加平稳。</li>
    </ul>
  </li>
  <li><strong>改善局部最小值问题</strong>：
    <ul>
      <li>在一些非凸问题中，梯度下降容易陷入局部最小值或鞍点。动量法通过引入历史梯度的加权平均，有时可以帮助模型摆脱这些不理想的局部最优，从而加速跳出鞍点。</li>
    </ul>
  </li>
</ol>

<h3 id="动量因子-beta">动量因子 ($\beta$)</h3>

<ul>
  <li><strong>($\beta$)</strong> 是动量法中的一个重要超参数，通常取值在 (0 &lt; $\beta$ &lt; 1) 之间。它决定了历史梯度对当前梯度更新的影响程度。常用的值是 0.9 或 0.99。
    <ul>
      <li><strong>($\beta$ = 0.9)</strong>：这意味着历史梯度的影响占比为 90%，当前梯度的影响占比为 10%。</li>
      <li><strong>($\beta$) 较小</strong>（例如 0.5）：历史梯度的影响较小，更倾向于依赖当前的梯度。</li>
      <li><strong>($\beta$) 较大</strong>（接近 1，例如 0.99）：历史梯度的影响较大，这对于减少训练中的震荡尤为重要。</li>
    </ul>
  </li>
</ul>

<h3 id="动量法的优势">动量法的优势</h3>

<ol>
  <li><strong>加速收敛</strong>：
    <ul>
      <li>动量法能够帮助优化器加速收敛，尤其在训练过程中梯度变化较小的情况下，能够加速向最优解收敛。</li>
    </ul>
  </li>
  <li><strong>避免震荡</strong>：
    <ul>
      <li>在面对高曲率或震荡的区域时，动量法通过历史梯度的“记忆”能够平滑梯度的波动，避免了在梯度较大的方向上的不必要调整。</li>
    </ul>
  </li>
  <li><strong>适应性强</strong>：
    <ul>
      <li>对于许多神经网络的训练，动量法常常能在优化过程中比标准的梯度下降法表现得更好，尤其是对于复杂的损失函数（如深度神经网络中的损失函数）。</li>
    </ul>
  </li>
</ol>

<h3 id="动量法的缺点">动量法的缺点</h3>

<ol>
  <li><strong>超参数调节</strong>：
    <ul>
      <li>动量因子 ($\beta$) 和学习率 ($\alpha$) 的选择对训练过程有较大影响。需要通过交叉验证等方法调优。</li>
    </ul>
  </li>
  <li><strong>可能导致过冲</strong>：
    <ul>
      <li>如果 ($\beta$) 值过大，动量法可能会引发过冲（overshooting），使得模型的更新在某些情况下过于剧烈，导致优化不稳定。</li>
    </ul>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># 动量法
</span><span class="n">w</span> <span class="o">+=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span>  <span class="c1"># 梯度影响速度
</span></pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="动量法的变种">动量法的变种</h3>

<ol>
  <li><strong>Nesterov加速梯度（NAG, Nesterov Accelerated Gradient）</strong>：
    <ul>
      <li>
        <p>Nesterov 动量是对动量法的一种改进，在计算梯度时先进行预估，即在当前的梯度更新基础上计算一个小的步伐，然后再计算新的梯度。这种方法在更新时比标准动量法更加精确，并且收敛速度可能更快。</p>
      </li>
      <li>
        <p>Nesterov加速梯度的更新公式：
\(v_{t+1} = \beta v_t + (1 - \beta) \nabla L(\theta_t - \alpha \beta v_t)\)</p>

        <p>这种方法通过先使用部分更新方向来预估梯度，从而在梯度计算时就更精确。</p>
      </li>
    </ul>
  </li>
</ol>

:ET