I"<blockquote>
  <p>相对熵（<code class="language-plaintext highlighter-rouge">Relative Entropy</code>）和交叉熵（<code class="language-plaintext highlighter-rouge">Cross Entropy</code>）是信息论中的两个重要概念，在机器学习中，它们通常用来衡量两个概率分布之间的差异</p>
</blockquote>

<h3 id="一相对熵relative-entropy">一、相对熵（Relative Entropy）</h3>

<p><strong>定义：</strong>
相对熵，也被称为 <strong>Kullback-Leibler Divergence (KL 散度)</strong>，是度量两个概率分布之间差异的非对称度量。给定两个概率分布 ($P$) 和 ($Q$)，相对熵 ($D_{KL}(P || Q)$) 衡量的是在假设数据来自分布 ($Q$) 的情况下，使用分布 ($P$) 来编码信息时的额外“代价”或“误差”。其数学表达式为：
\(D_{KL}(P || Q) = \sum_{x} P(x) \log \left(\frac{P(x)}{Q(x)}\right)\)
<strong>性质：</strong></p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>非负性</strong>：相对熵总是大于或等于零，即 ($D_{KL}(P</td>
          <td> </td>
          <td>Q) \geq 0$)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>非对称性</strong>：($D_{KL}(P</td>
          <td> </td>
          <td>Q)$) 不等于 ($D_{KL}(Q</td>
          <td> </td>
          <td>P)$)，因此相对熵不是度量<code class="language-plaintext highlighter-rouge">metric</code>，也就是不表示距离。</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>解释：</strong></p>

<ul>
  <li>相对熵衡量的是使用一个分布 (Q) 来逼近另一个分布 (P) 时，额外的编码代价或信息损失。简单来说，它衡量了 <strong>真实分布 (P)</strong> 和 <strong>近似分布 (Q)</strong> 之间的“差异”或“误差”</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>当 ($P = Q$) 时，($D_{KL}(P</td>
          <td> </td>
          <td>Q) = 0$)，意味着两个分布完全相同</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>主要应用在<strong>生成模型</strong>和 <strong>变分推理</strong> 中，例如变分自编码器（VAE）</p>

<hr />

<h3 id="二交叉熵cross-entropy">二、交叉熵（Cross Entropy）</h3>

<p><strong>定义：</strong>
交叉熵是一种常用的损失函数，用来度量两个概率分布之间的差异 它衡量的是使用概率分布 (Q) 来编码真实分布 (P) 时的“信息量”。在二分类和多分类问题中，交叉熵通常用来衡量预测分布与真实标签分布之间的差异。</p>

<p>对于两个概率分布 ($P$) 和 ($Q$)，交叉熵 ($H(P, Q)$) 的定义为：</p>

\[H(P, Q) = - \sum_{x} P(x) \log Q(x)\]

<p>其中：</p>
<ul>
  <li>($P(x)$) 是真实分布（即数据的真实标签分布）。</li>
  <li>($Q(x)$) 是预测分布（即模型预测的标签分布）。</li>
  <li>($\sum_{x}$) 是对所有可能的 ($x$) 求和。</li>
</ul>

<p><strong>解释：</strong></p>
<ul>
  <li>交叉熵实际上是基于<strong>KL 散度</strong>的定义的。它可以看作是 <strong>真实分布 (P)</strong> 与 <strong>预测分布 (Q)</strong> 之间的 <strong>期望相对熵</strong>。</li>
  <li>如果 (P = Q)，交叉熵是最小的，且值为 ($-\sum_{x} P(x) \log P(x)$)，即真实分布的自熵。</li>
</ul>

<p><strong>应用：</strong>
交叉熵是分类问题中常用的损失函数。在深度学习中，它被广泛应用于二分类和多分类任务中：</p>
<ul>
  <li>
    <p>在二分类问题中，交叉熵损失通常是：</p>

\[L_{\text{binary}}(P, Q) = - \left( y \log(p) + (1 - y) \log(1 - p) \right)\]

    <p>其中 ($y$) 是真实标签，($p$) 是模型预测的概率。</p>
  </li>
  <li>
    <p>在多分类问题中，交叉熵损失是：</p>

\[L_{\text{categorical}}(P, Q) = - \sum_{i} y_i \log(p_i)\]

    <p>其中 ($y_i$) 是真实标签的概率分布，($p_i$) 是模型预测的概率。</p>
  </li>
</ul>

<hr />

<h3 id="三相对熵与交叉熵的关系">三、相对熵与交叉熵的关系</h3>

<blockquote>
  <p>从数学角度来看，交叉熵和相对熵是密切相关的。实际上，交叉熵可以分解为<strong>真实分布的自熵</strong>和<strong>真实分布与预测分布之间的相对熵</strong>：
\(D_{KL}(P || Q) = \sum_{x} P(x) \log \left(P(x)\right)-\sum_{x} P(x) \log \left(Q(x)\right)=-H(P)+H(P,Q)\)
其中$H(P,Q)$表示$P$和$Q$的交叉熵，$H(P)$表示概率分布$P$的自熵（基于$P$分布自身的编码长度，也就是最优的编码长度）</p>
</blockquote>

\[D_{KL}(P || Q) + H(P)=H(P,Q)\]

<p>因此，交叉熵包含了两部分：</p>
<ol>
  <li>真实分布的自熵（其值是固定的，取决于数据）。</li>
  <li>相对熵，衡量模型预测与真实分布之间的差异。</li>
</ol>

<p>交叉熵的最小化实际上是在最小化相对熵（因此自熵固定），因此通过最小化交叉熵，我们也在优化模型，使得它的预测分布尽可能接近真实分布。</p>

<hr />

<h3 id="四总结">四、总结</h3>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>相对熵（KL散度）</strong>：衡量两个概率分布之间的差异，度量了真实分布与预测分布之间的额外编码代价。它是一个非对称的“度量”（不是度量），广泛应用于生成模型和变分推理中，<code class="language-plaintext highlighter-rouge">pytorch</code>中<code class="language-plaintext highlighter-rouge">F.kl_div(Q.log(), P, reduction='')</code>表示$D_{KL}(P</td>
          <td> </td>
          <td>Q)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>交叉熵</strong>：衡量真实分布与预测分布之间的差异，常用作分类任务中的损失函数。它可以看作是真实分布的自熵与相对熵之和，<code class="language-plaintext highlighter-rouge">pytorch</code>中<code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss()</code></p>
  </li>
  <li><strong>关系</strong>：交叉熵由真实分布的自熵和真实分布与预测分布之间的相对熵组成。在优化时，交叉熵的最小化意味着我们在尽量减少模型预测和真实分布之间的差异。</li>
</ul>
:ET