I"<blockquote>
  <p><strong>梯度优化算法发展历史</strong></p>

  <p><code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>SGD -&gt; SGDM -&gt;NAG -&gt; AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam
</pre></td></tr></tbody></table></code></p>

  <ul>
    <li><code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>SGD
</pre></td></tr></tbody></table></code>：利用参数梯度和学习率调整梯度</li>
    <li><code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Momentum
</pre></td></tr></tbody></table></code>：加入了历史梯度信息，将使用之前梯度的加权平均来更新当前的参数</li>
    <li><code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>RMSprop
</pre></td></tr></tbody></table></code>：通过梯度的平方的均值（梯度的波动情况）来调整每个参数的学习率</li>
    <li><code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Adam
</pre></td></tr></tbody></table></code>: 将<code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Momentum
</pre></td></tr></tbody></table></code>和<code class="highlighter-rouge"><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>RMSprop
</pre></td></tr></tbody></table></code>优点结合</li>
  </ul>
</blockquote>

:ET