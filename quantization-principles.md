---
layout: markdown
title: "量化的核心原理"
description: "深入讲解神经网络量化的数学原理、权重量化运算机制"
date: 2024-08-03
tags: ["Qwen", "量化", "数学原理", "深度学习"]
category: "LLM"
permalink: /quantization-principles/
---

# 量化的核心原理：浮点数到整数的映射

量化的基本思想是将神经网络中通常使用的浮点数（如FP32，32位浮点数）表示的权重和激活值，映射到低位宽的整数（如INT8，8位整数）表示。这个映射过程需要定义一个**量化函数**和一个**反量化函数**。

最常见的量化方法是**线性均匀量化 (Linear Uniform Quantization)**，它通过一个**缩放因子 (Scale Factor, S)** 和一个**零点 (Zero Point, Z)** 来实现浮点数和整数之间的线性映射。

## 1. 量化参数 (S 和 Z) 的确定

在进行量化之前，我们需要确定浮点数范围（$F_{min}$,$F_{max}$）和整数范围（$Q_{min}$,$Q_{max}$）。

- **整数范围**通常是固定的。例如，对于 8 位无符号整数 `(UINT8)` $Q_{min}=0$,$Q_{max}=255$；对于 8 位有符号整数 `(INT8)`，$Q_{min}=-128$,$Q_{max}=127$。
- **浮点数范围**则需要从模型中获取。

一旦确定了浮点数的范围，就可以计算 S 和 Z：

- 缩放因子 S (Scale Factor):
  $$
  S=(Q_{max}−Q_{min})/ (F_{max}−F_{min})
  $$

- 零点 Z (Zero Point):
  $$
  Z=Q_{min}−S \\times F_{min}
  $$

## 2. 量化过程 (Quantization)

将一个浮点数 FP 量化为整数 Q 的公式：

$$
Q=round(S \\times FP+Z)
$$

其中 round 表示四舍五入。

## 3. 反量化过程 (Dequantization)

在某些情况下，为了进行计算或输出，需要将整数 Q 反量化回浮点数：

$$
FP_{reconstructed}=(Q−Z) \\times S
$$

**为什么需要零点 Z？**

零点的存在是为了确保浮点数 0.0 能够精确地映射到一个整数值，这对于激活函数（如 ReLU，其中大量值为 0）和稀疏权重矩阵非常重要，可以避免量化误差累积。

## 权重量化运算

模型参数（权重）的量化比激活值相对简单，因为权重是固定的。主要有两种策略：

### 离线量化 (PTQ - Post-Training Quantization)

这是最常见的权重量化方式：

1. 加载预训练的 FP32 模型
2. 计算每个权重张量的最小值和最大值
3. 根据范围计算对应的 S 和 Z
4. 将每个浮点权重量化为整数
5. 存储量化后的模型

### 在线量化 (QAT - Quantization-Aware Training)

在训练过程中引入量化操作的模拟：

1. 在训练图中插入伪量化节点
2. 执行量化→反量化操作来模拟量化误差
3. 模型学习适应量化带来的误差
4. 部署时直接进行最终量化

---

📚 **相关文章推荐：**
- [Llama系列大模型解析](/llama-series/)
- [DeepSeek模型介绍](/deepseek-intro/)
- [回到博客首页](/blog/)